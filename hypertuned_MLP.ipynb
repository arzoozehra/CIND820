{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arzoozehra/CIND820/blob/main/hypertuned_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import libraries**"
      ],
      "metadata": {
        "id": "-bkY_pYm8WwM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Wr2dHIs7WLWJ",
        "outputId": "800f164e-74e6-4ad2-cf6a-16fd17b7b5a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.8/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.8/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.8/dist-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.8/dist-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "!pip install contractions\n",
        "import contractions\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "#!pip install pyspellchecker\n",
        "#from spellchecker import SpellChecker\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "#from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load data**"
      ],
      "metadata": {
        "id": "GfYAFFPs8gNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/arzoozehra/CIND820/main/data/train.csv\"\n",
        "train = pd.read_csv(url)\n",
        "test = pd.read_csv(\"https://raw.githubusercontent.com/arzoozehra/CIND820/main/data/test.csv\")\n",
        "\n",
        "train.drop([\"textID\", \"selected_text\"], axis=1, inplace=True)\n",
        "test.drop([\"textID\"], axis=1, inplace=True)\n",
        "\n",
        "# Remove row with missing values\n",
        "train.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "WzPmmeuGrev1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train[\"text\"].head(10))\n",
        "print(train[\"text\"].tail(10))"
      ],
      "metadata": {
        "id": "rQwxYsauWPT1",
        "outputId": "58d76feb-6d8a-4595-c6c0-827f884486a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                  I`d have responded, if I were going\n",
            "1        Sooo SAD I will miss you here in San Diego!!!\n",
            "2                            my boss is bullying me...\n",
            "3                       what interview! leave me alone\n",
            "4     Sons of ****, why couldn`t they put them on t...\n",
            "5    http://www.dothebouncy.com/smf - some shameles...\n",
            "6    2am feedings for the baby are fun when he is a...\n",
            "7                                           Soooo high\n",
            "8                                          Both of you\n",
            "9     Journey!? Wow... u just became cooler.  hehe....\n",
            "Name: text, dtype: object\n",
            "27471    i`m defying gravity. and nobody in alll of oz,...\n",
            "27472    http://twitpic.com/663vr - Wanted to visit the...\n",
            "27473     in spoke to you yesterday and u didnt respond...\n",
            "27474    So I get up early and I feel good about the da...\n",
            "27475                                       enjoy ur night\n",
            "27476     wish we could come see u on Denver  husband l...\n",
            "27477     I`ve wondered about rake to.  The client has ...\n",
            "27478     Yay good for both of you. Enjoy the break - y...\n",
            "27479                           But it was worth it  ****.\n",
            "27480       All this flirting going on - The ATG smiles...\n",
            "Name: text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clean data**"
      ],
      "metadata": {
        "id": "QbW4UiEpq9kA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(data):\n",
        "  \n",
        "  # Convert text to lowercase\n",
        "  data[\"text\"] = data[\"text\"].str.lower()\n",
        "\n",
        "  # Expand contractions e.g \"gonna\" to \"going to\" and \"i've\" to \"i have\"\n",
        "  data[\"text\"].replace( {r\"`\": \"'\"}, inplace= True, regex = True)\n",
        "  data[\"text\"] = data[\"text\"].apply(contractions.fix)\n",
        "\n",
        "  # Remove @, Unicode characters, punctuation, emojis, URLs, retweets, words with digits, and 1 or 2 letter words\n",
        "  data[\"text\"].replace( {r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?|\\w*\\d\\w*|\\b\\w{1,2}\\b\": \" \"}, inplace= True, regex = True)\n",
        "\n",
        "  # Remove extra whitespaces\n",
        "  data[\"text\"].replace( {r\" +\": \" \"}, inplace= True, regex = True)\n",
        "  data[\"text\"] = data[\"text\"].str.strip()\n",
        "\n",
        "  # Correct spellings\n",
        "  #spell = SpellChecker()\n",
        "\n",
        "  #def correct_spellings(text):\n",
        "  #    corrected_text = []\n",
        "  #    misspelled_words = {}\n",
        "  #    words = text.split()\n",
        "  #    for w in spell.unknown(words):\n",
        "  #        corr = spell.correction(w)\n",
        "  #        if corr:\n",
        "  #            misspelled_words[w] = spell.correction(w) or w\n",
        "  #    corrected_text = [misspelled_words.get(w, w) for w in words]\n",
        "  #    return \" \".join(corrected_text)\n",
        "\n",
        "  #data[\"text\"] = data[\"text\"].apply(lambda x : correct_spellings(x))\n",
        "\n",
        "  # Remove stopwords\n",
        "  stop = stopwords.words(\"english\")\n",
        "  data[\"text\"] = data[\"text\"].apply(lambda text: \" \".join([word for word in text.split() if word not in (stop)]))\n",
        "\n",
        "  # Stemming\n",
        "  stemmer = PorterStemmer()\n",
        "  data[\"text\"] = data[\"text\"].apply(lambda text: \" \".join([stemmer.stem(word) for word in text.split()]))\n",
        "\n",
        "  # Lemmatizing\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  data[\"text\"] = data[\"text\"].apply(lambda text: \" \".join([lemmatizer.lemmatize(word) for word in text.split()]))\n",
        "\n",
        "  return data"
      ],
      "metadata": {
        "id": "NyWCTuFDeTPP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean trraining data\n",
        "train = clean_data(train)\n",
        "\n",
        "#Clean testing data\n",
        "test = clean_data(test)"
      ],
      "metadata": {
        "id": "5tDnIMd-q89D"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train[\"text\"].head(10))\n",
        "print(train[\"text\"].tail(10))"
      ],
      "metadata": {
        "id": "tT-89PPBe703",
        "outputId": "6e026e60-19bb-4db1-caec-3bacea568a28",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                          would respond go\n",
            "1                   sooo sad miss san diego\n",
            "2                                 bos bulli\n",
            "3                       interview leav alon\n",
            "4       son could put releas alreadi bought\n",
            "5    shameless plug best ranger forum earth\n",
            "6                   feed babi fun smile coo\n",
            "7                                soooo high\n",
            "8                                          \n",
            "9     journey wow becam cooler hehe possibl\n",
            "Name: text, dtype: object\n",
            "27471        defi graviti nobodi alll wizard ever go bring\n",
            "27472                                 want visit anim late\n",
            "27473           spoke yesterday respond girl wassup though\n",
            "27474    get earli feel good day walk work feel alright...\n",
            "27475                                          enjoy night\n",
            "27476    wish could come see denver husband lost job ca...\n",
            "27477    wonder rake client made clear net forc dev lea...\n",
            "27478    yay good enjoy break probabl need hectic weeke...\n",
            "27479                                                worth\n",
            "27480                           flirt go atg smile yay hug\n",
            "Name: text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Feature Selection**"
      ],
      "metadata": {
        "id": "SRySCY_E8Kke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorization parameters\n",
        "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
        "NGRAM_RANGE = (1, 2)  # Use 1-grams + 2-grams.\n",
        "\n",
        "# Whether text should be split into word or character n-grams.\n",
        "TOKEN_MODE = \"word\" # Split text into word tokens.\n",
        "\n",
        "# Minimum document frequency below which a token will be discarded.\n",
        "MIN_DOCUMENT_FREQUENCY = 5\n",
        "\n",
        "\n",
        "def ngram_vectorize(train_texts, train_labels, test_texts):\n",
        "    \"\"\"Vectorizes texts as n-gram vectors.\n",
        "\n",
        "    1 text = 1 tf-idf vector the length of vocabulary of unigrams + bigrams.\n",
        "\n",
        "    # Arguments\n",
        "        train_texts: list, training text strings.\n",
        "        train_labels: np.ndarray, training labels.\n",
        "        test_texts: list, test text strings.\n",
        "\n",
        "    # Returns\n",
        "        train_vectors, test_vectors: vectorized training and test texts\n",
        "    \"\"\"\n",
        "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
        "    kwargs = {\n",
        "            \"ngram_range\": NGRAM_RANGE,\n",
        "            \"analyzer\": TOKEN_MODE,  \n",
        "            \"min_df\": MIN_DOCUMENT_FREQUENCY,\n",
        "            \"max_df\" : 0.8,\n",
        "            \"sublinear_tf\": \"True\"\n",
        "    }\n",
        "    vectorizer = TfidfVectorizer(**kwargs)\n",
        "\n",
        "    # Learn vocabulary from training texts and vectorize training texts.\n",
        "    train_vectors = vectorizer.fit_transform(train_texts)\n",
        "\n",
        "    # Vectorize validation texts.\n",
        "    test_vectors = vectorizer.transform(test_texts)\n",
        "\n",
        "    return train_vectors, test_vectors"
      ],
      "metadata": {
        "id": "I3KHg2k7kGyh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Create feature vectors\n",
        "train_vectors, test_vectors = ngram_vectorize(train[\"text\"], train[\"sentiment\"], test[\"text\"])"
      ],
      "metadata": {
        "id": "JxxnENgQbT8j"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MLP modelling and evaluation**\n"
      ],
      "metadata": {
        "id": "qSC6vhwKaAb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MLPClassifier(random_state=42)\n",
        "model_name = \"MLP\"\n",
        "\n",
        "# 5-fold Cross-validation\n",
        "k = 5\n",
        "cv_df = pd.DataFrame(index=range(k))\n",
        "entries = []\n",
        "accuracies = cross_val_score(model, train_vectors, train[\"sentiment\"], scoring=\"accuracy\", cv=k)\n",
        "for fold_id, accuracy in enumerate(accuracies):\n",
        "  entries.append((fold_id, accuracy))\n",
        "\n",
        "cv_df = pd.DataFrame(entries, columns=[\"fold_id\", \"accuracy\"])"
      ],
      "metadata": {
        "id": "wfQ4nX-DdPtN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6686712c-14ab-4def-93a6-8231e5b56216"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:699: UserWarning: Training interrupted by user.\n",
            "  warnings.warn(\"Training interrupted by user.\")\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:699: UserWarning: Training interrupted by user.\n",
            "  warnings.warn(\"Training interrupted by user.\")\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:699: UserWarning: Training interrupted by user.\n",
            "  warnings.warn(\"Training interrupted by user.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_accuracy = cv_df.accuracy.mean()\n",
        "std_accuracy = cv_df.accuracy.std()\n",
        "\n",
        "acc = pd.DataFrame([[mean_accuracy,std_accuracy]], columns = [\"Accuracy\", \" Std dev\"])\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "TE3JzHbFR2lC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = MLPClassifier(hidden_layer_sizes=(256,128,64,32),activation=\"relu\",random_state=42)\n",
        "# model_name = \"MLP\""
      ],
      "metadata": {
        "id": "72nZxxJtNggk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xG3urTN6PZ1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction = model.predict(test_vectors)\n",
        "# print(model.score(test_vectors, test['sentiment']))\n",
        "# print(f\"Test set accuracy: {accuracy_score(test['sentiment'], prediction) * 100} %\\n\")"
      ],
      "metadata": {
        "id": "M5Xy2YcYN9QN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_vectors, train[\"sentiment\"])\n",
        "prediction = model.predict(test_vectors)\n",
        "print(f\"Test set accuracy: {accuracy_score(test['sentiment'], prediction) * 100} %\\n\")"
      ],
      "metadata": {
        "id": "8TXDJnAAjs3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report\n",
        "print(f\"\\tCLASSIFICATIION METRICS - {model_name}\\n\")\n",
        "print(classification_report(test[\"sentiment\"], prediction, target_names= [\"negative\", \"neutral\", \"positive\"]))"
      ],
      "metadata": {
        "id": "IsfG8KcKj3g0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = confusion_matrix(test[\"sentiment\"], prediction)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=data, display_labels=model.classes_)\n",
        "disp.plot(cmap=\"Blues\")\n",
        "plt.ylabel(\"ACTUAL\")\n",
        "plt.xlabel(\"\\nPREDICTED\")\n",
        "plt.title(f\"\\nCONFUSION MATRIX - {model_name}\\n\");\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LXYBbBiHkFtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypertuning the model parameters**"
      ],
      "metadata": {
        "id": "i_8q07ADycg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\"hidden_layer_sizes\": [(50,50,50), (50,100,50), (256,128,64,32)],\n",
        "              \"alpha\": [0.0001, 0.05],\n",
        "              \"learning_rate\": [\"constant\",\"adaptive\"],\n",
        "             }"
      ],
      "metadata": {
        "id": "eBGX65Zvsoff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_search = RandomizedSearchCV(SVC(), param_grid, refit = True, verbose = 5)\n",
        "random_search.fit(train_vectors, train[\"sentiment\"])"
      ],
      "metadata": {
        "id": "v5OaE5rJspWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(random_search.best_params_)"
      ],
      "metadata": {
        "id": "WlWY9R2zuSJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(random_search.best_estimator_)"
      ],
      "metadata": {
        "id": "Y3y8TdviuTqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(random_search.best_score_)"
      ],
      "metadata": {
        "id": "h53_h0A74Jba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluating the model with best parameters**"
      ],
      "metadata": {
        "id": "c2AmW1a5yw2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_predictions = random_search.best_estimator_.predict(train_vectors)\n",
        "print(accuracy_score(train[\"sentiment\"], random_predictions))"
      ],
      "metadata": {
        "id": "riLXlVEGuYSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_predictions = random_search.best_estimator_.predict(test_vectors)\n",
        "print(accuracy_score(test[\"sentiment\"], random_predictions))"
      ],
      "metadata": {
        "id": "AlNEJ3SM4n0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(test[\"sentiment\"], random_predictions))"
      ],
      "metadata": {
        "id": "M6JgIYiruvHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(test[\"sentiment\"], random_predictions))"
      ],
      "metadata": {
        "id": "CYt8fTJfyMaD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}