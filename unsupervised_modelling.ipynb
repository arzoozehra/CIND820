{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arzoozehra/CIND820/blob/main/unsupervised_modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import libraries**"
      ],
      "metadata": {
        "id": "-bkY_pYm8WwM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Wr2dHIs7WLWJ",
        "outputId": "9af17511-937c-4c13-d850-2819578b592c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 5.6 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 31.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.1 contractions-0.1.73 pyahocorasick-1.4.4 textsearch-0.0.24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "!pip install contractions\n",
        "import contractions\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "#!pip install pyspellchecker\n",
        "#from spellchecker import SpellChecker\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "#from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from tensorflow.python.keras import models\n",
        "from tensorflow.python.keras.layers import Dense, Dropout\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping\n",
        "from tensorflow.python.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.python.keras.optimizer_v2.adam import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load data**"
      ],
      "metadata": {
        "id": "GfYAFFPs8gNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/arzoozehra/CIND820/main/data/train.csv\"\n",
        "train = pd.read_csv(url)\n",
        "test = pd.read_csv(\"https://raw.githubusercontent.com/arzoozehra/CIND820/main/data/test.csv\")\n",
        "\n",
        "train.drop([\"textID\", \"selected_text\"], axis=1, inplace=True)\n",
        "test.drop([\"textID\"], axis=1, inplace=True)\n",
        "\n",
        "# Remove row with missing values\n",
        "train.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "WzPmmeuGrev1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clean data**"
      ],
      "metadata": {
        "id": "QbW4UiEpq9kA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(data):\n",
        "  \n",
        "  # Convert text to lowercase\n",
        "  data[\"text\"] = data[\"text\"].str.lower()\n",
        "\n",
        "  # Expand contractions e.g \"gonna\" to \"going to\" and \"i've\" to \"i have\"\n",
        "  data[\"text\"].replace( {r\"`\": \"'\"}, inplace= True, regex = True)\n",
        "  data[\"text\"] = data[\"text\"].apply(contractions.fix)\n",
        "\n",
        "  # Remove @, Unicode characters, punctuation, emojis, URLs, retweets, words with digits, and 1 or 2 letter words\n",
        "  data[\"text\"].replace( {r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?|\\w*\\d\\w*|\\b\\w{1,2}\\b\": \" \"}, inplace= True, regex = True)\n",
        "\n",
        "  # Remove extra whitespaces\n",
        "  data[\"text\"].replace( {r\" +\": \" \"}, inplace= True, regex = True)\n",
        "  data[\"text\"] = data[\"text\"].str.strip()\n",
        "\n",
        "  # Correct spellings\n",
        "  #spell = SpellChecker()\n",
        "\n",
        "  #def correct_spellings(text):\n",
        "  #    corrected_text = []\n",
        "  #    misspelled_words = {}\n",
        "  #    words = text.split()\n",
        "  #    for w in spell.unknown(words):\n",
        "  #        corr = spell.correction(w)\n",
        "  #        if corr:\n",
        "  #            misspelled_words[w] = spell.correction(w) or w\n",
        "  #    corrected_text = [misspelled_words.get(w, w) for w in words]\n",
        "  #    return \" \".join(corrected_text)\n",
        "\n",
        "  #data[\"text\"] = data[\"text\"].apply(lambda x : correct_spellings(x))\n",
        "\n",
        "  # Remove stopwords\n",
        "  stop = stopwords.words(\"english\")\n",
        "  data[\"text\"] = data[\"text\"].apply(lambda text: \" \".join([word for word in text.split() if word not in (stop)]))\n",
        "\n",
        "  # Stemming\n",
        "  stemmer = PorterStemmer()\n",
        "  data[\"text\"] = data[\"text\"].apply(lambda text: \" \".join([stemmer.stem(word) for word in text.split()]))\n",
        "\n",
        "  # Lemmatizing\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  data[\"text\"] = data[\"text\"].apply(lambda text: \" \".join([lemmatizer.lemmatize(word) for word in text.split()]))\n",
        "\n",
        "  return data"
      ],
      "metadata": {
        "id": "5tDnIMd-q89D"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean trraining data\n",
        "train = clean_data(train)\n",
        "\n",
        "#Clean testing data\n",
        "test = clean_data(test)"
      ],
      "metadata": {
        "id": "lil3NvKYD-3j"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_num_classes(labels):\n",
        "    \"\"\"Gets the total number of classes.\n",
        "    # Arguments\n",
        "        labels: list, label values.\n",
        "            There should be at lease one sample for values in the\n",
        "            range (0, num_classes -1)\n",
        "    # Returns\n",
        "        int, total number of classes.\n",
        "    # Raises\n",
        "        ValueError: if any label value in the range(0, num_classes - 1)\n",
        "            is missing or if number of classes is <= 1.\n",
        "    \"\"\"\n",
        "    num_classes = max(labels) + 1\n",
        "    missing_classes = [i for i in range(num_classes) if i not in labels]\n",
        "    if len(missing_classes):\n",
        "        raise ValueError('Missing samples with label value(s) '\n",
        "                         '{missing_classes}. Please make sure you have '\n",
        "                         'at least one sample for every label value '\n",
        "                         'in the range(0, {max_class})'.format(\n",
        "                            missing_classes=missing_classes,\n",
        "                            max_class=num_classes - 1))\n",
        "\n",
        "    if num_classes <= 1:\n",
        "        raise ValueError('Invalid number of labels: {num_classes}.'\n",
        "                         'Please make sure there are at least two classes '\n",
        "                         'of samples'.format(num_classes=num_classes))\n",
        "    return num_classes\n"
      ],
      "metadata": {
        "id": "15cg2iGGlFZd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Feature Selection**"
      ],
      "metadata": {
        "id": "SRySCY_E8Kke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorization parameters\n",
        "\n",
        "def ngram_vectorize(train_texts, train_labels, val_texts):\n",
        "  \"\"\"Vectorizes texts as ngram vectors.\n",
        "  1 text = 1 tf-idf vector the length of vocabulary of uni-grams + bi-grams.\n",
        "  # Arguments\n",
        "      train_texts: list, training text strings.\n",
        "      train_labels: np.ndarray, training labels.\n",
        "      val_texts: list, validation text strings.\n",
        "  # Returns\n",
        "      x_train, x_val: vectorized training and validation texts\n",
        "  \"\"\"\n",
        "  # Range (inclusive) of n-gram sizes for tokenizing text.\n",
        "  # Use 1-grams + 2-grams.\n",
        "  NGRAM_RANGE = (1, 2)\n",
        "\n",
        "  # Whether text should be split into word or character n-grams.\n",
        "  # Split text into word tokens.\n",
        "  TOKEN_MODE = 'word'\n",
        "\n",
        "  # Minimum document/corpus frequency below which a token will be discarded.\n",
        "  MIN_DOCUMENT_FREQUENCY = 5\n",
        "\n",
        "  # Limit on the number of features. We use the top 20K features.\n",
        "  TOP_K = 20000\n",
        "\n",
        "  # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
        "  kwargs = {\n",
        "          \"ngram_range\": NGRAM_RANGE,\n",
        "          \"analyzer\": TOKEN_MODE,  \n",
        "          \"min_df\": MIN_DOCUMENT_FREQUENCY,\n",
        "          \"max_df\" : 0.8,\n",
        "          \"sublinear_tf\": \"True\"\n",
        "  }\n",
        "  vectorizer = TfidfVectorizer(**kwargs)\n",
        "\n",
        "  # Learn vocabulary from training texts and vectorize training texts.\n",
        "  x_train = vectorizer.fit_transform(train_texts)\n",
        "\n",
        "  # Vectorize validation texts.\n",
        "  x_val = vectorizer.transform(val_texts)\n",
        "\n",
        "  # Select top 'k' of the vectorized features.\n",
        "  selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
        "  selector.fit(x_train, train_labels)\n",
        "  x_train = selector.transform(x_train)\n",
        "  x_val = selector.transform(x_val)\n",
        "\n",
        "  x_train = x_train.toarray()\n",
        "  x_val = x_val.toarray()\n",
        "  return x_train, x_val"
      ],
      "metadata": {
        "id": "eVC51r3aO6-6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(np.shape(x_train))\n",
        "#print(np.shape(x_val))\n",
        "# print(type(train_vectors))\n",
        "# print(type(test_vectors))\n",
        "# print(train_vectors)\n",
        "# print(test_vectors)"
      ],
      "metadata": {
        "id": "2EUPVYY3EF8S"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unsupervised modelling using TensorFlow**"
      ],
      "metadata": {
        "id": "fp5HLVdNboZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp_model(layers, units, dropout_rate, input_shape, op_units=3, op_activation='softmax'):\n",
        "    \"\"\"Creates an instance of a multi-layer perceptron model.\n",
        "\n",
        "    # Arguments\n",
        "        layers: int, number of `Dense` layers in the model.\n",
        "        units: int, output dimension of the layers.\n",
        "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
        "        input_shape: tuple, shape of input to the model.\n",
        "        op_units: int, number of output classes.\n",
        "        op_activation: softmax for multiclass\n",
        "\n",
        "    # Returns\n",
        "        An MLP model instance.\n",
        "    \"\"\"\n",
        "\n",
        "    model = models.Sequential()\n",
        "    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n",
        "\n",
        "    for _ in range(layers-1):\n",
        "        model.add(Dense(units=units, activation='relu'))\n",
        "        model.add(Dropout(rate=dropout_rate))\n",
        "\n",
        "    model.add(Dense(units=op_units, activation=op_activation))\n",
        "    return model"
      ],
      "metadata": {
        "id": "lyYc_8N3bkb7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train model**"
      ],
      "metadata": {
        "id": "0ujbGFwieJlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate=1e-3\n",
        "epochs=1000\n",
        "batch_size=128\n",
        "layers=2\n",
        "units=64\n",
        "dropout_rate=0.2\n",
        "\n",
        "\"\"\"Trains n-gram model on the given dataset.\n",
        "\n",
        "# Arguments\n",
        "    train, test: tuples of training and test texts and labels.\n",
        "    learning_rate: float, learning rate for training model.\n",
        "    epochs: int, number of epochs.\n",
        "    batch_size: int, number of samples per batch.\n",
        "    layers: int, number of `Dense` layers in the model.\n",
        "    units: int, output dimension of Dense layers in the model.\n",
        "    dropout_rate: float: percentage of input to drop at Dropout layers.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(train[\"text\"], train[\"sentiment\"], test_size=0.2, random_state=42)\n",
        "\n",
        "# Encode train and test labels\n",
        "le = LabelEncoder()\n",
        "train_labels = le.fit_transform(train_labels)\n",
        "val_labels = le.fit_transform(val_labels)\n",
        "\n",
        "# Verify that validation labels are in the same range as training labels.\n",
        "num_classes = get_num_classes(train_labels)\n",
        "unexpected_labels = [v for v in val_labels if v not in range(num_classes)]\n",
        "if len(unexpected_labels):\n",
        "    raise ValueError('Unexpected label values found in the validation set:'\n",
        "                      ' {unexpected_labels}. Please make sure that the '\n",
        "                      'labels in the validation set are in the same range '\n",
        "                      'as training labels.'.format(\n",
        "                          unexpected_labels=unexpected_labels))\n",
        "\n",
        "\n",
        "# Vectorize texts.\n",
        "x_train, x_val = ngram_vectorize(train_texts, train_labels, val_texts)\n",
        "\n",
        "# Create model instance.\n",
        "model = mlp_model(layers=layers,\n",
        "                  units=units,\n",
        "                  dropout_rate=dropout_rate,\n",
        "                  input_shape=x_train.shape[1:])\n",
        "\n",
        "# Compile model with learning parameters.\n",
        "loss = 'sparse_categorical_crossentropy'\n",
        "optimizer = Adam(learning_rate=learning_rate)\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
        "\n",
        "# Create callback for early stopping on validation loss. If the loss does\n",
        "# not decrease in two consecutive tries, stop training.\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience=2)]\n",
        "\n",
        "# Train and validate model.\n",
        "history = model.fit(x_train, train_labels,\n",
        "                    epochs = epochs,\n",
        "                    callbacks = callbacks,\n",
        "                    validation_data = (x_val, val_labels),\n",
        "                    verbose = 2,  # Logs once per epoch.\n",
        "                    batch_size = batch_size)\n",
        "\n",
        "# Print results.\n",
        "history = history.history\n",
        "print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
        "        acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
        "\n",
        "# Save model.\n",
        "model.save('imdb_mlp_model.h5')\n",
        "#print(history['val_acc'][-1], history['val_loss'][-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0ySmmzIdhdS",
        "outputId": "568ba92b-a7fa-424a-f418-346ca08c7a0d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "172/172 - 4s - loss: 0.9922 - acc: 0.5208 - val_loss: 0.8567 - val_acc: 0.6439\n",
            "Epoch 2/1000\n",
            "172/172 - 3s - loss: 0.7812 - acc: 0.6728 - val_loss: 0.7597 - val_acc: 0.6774\n",
            "Epoch 3/1000\n",
            "172/172 - 3s - loss: 0.7025 - acc: 0.7054 - val_loss: 0.7448 - val_acc: 0.6845\n",
            "Epoch 4/1000\n",
            "172/172 - 3s - loss: 0.6652 - acc: 0.7253 - val_loss: 0.7434 - val_acc: 0.6816\n",
            "Epoch 5/1000\n",
            "172/172 - 3s - loss: 0.6415 - acc: 0.7303 - val_loss: 0.7492 - val_acc: 0.6779\n",
            "Epoch 6/1000\n",
            "172/172 - 3s - loss: 0.6282 - acc: 0.7403 - val_loss: 0.7605 - val_acc: 0.6745\n",
            "Validation accuracy: 0.6744905114173889, loss: 0.7605029940605164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_ngram_model(train, test,\n",
        "#                       learning_rate=1e-3,\n",
        "#                       epochs=100,\n",
        "#                       batch_size=128,\n",
        "#                       layers=2,\n",
        "#                       units=64,\n",
        "#                       dropout_rate=0.2):\n",
        "#     \"\"\"Trains n-gram model on the given dataset.\n",
        "\n",
        "#     # Arguments\n",
        "#         train, test: tuples of training and test texts and labels.\n",
        "#         learning_rate: float, learning rate for training model.\n",
        "#         epochs: int, number of epochs.\n",
        "#         batch_size: int, number of samples per batch.\n",
        "#         layers: int, number of `Dense` layers in the model.\n",
        "#         units: int, output dimension of Dense layers in the model.\n",
        "#         dropout_rate: float: percentage of input to drop at Dropout layers.\n",
        "#     \"\"\"\n",
        "\n",
        "\n",
        "#     #(train_texts, train_labels), (val_texts, val_labels) = data\n",
        "\n",
        "#     # Vectorize texts.\n",
        "#     x_train, x_val = ngram_vectorize(train[\"text\"], train[\"sentiment\"], test[\"text\"])\n",
        "    \n",
        "#     # Create model instance.\n",
        "#     model = mlp_model(layers=layers,\n",
        "#                       units=units,\n",
        "#                       dropout_rate=dropout_rate,\n",
        "#                       input_shape=x_train.shape[1:])\n",
        "\n",
        "#     # Compile model with learning parameters.\n",
        "#     loss = 'sparse_categorical_crossentropy'\n",
        "#     optimizer = Adam(lr=learning_rate)\n",
        "#     model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
        "\n",
        "#     # Create callback for early stopping on validation loss. If the loss does\n",
        "#     # not decrease in two consecutive tries, stop training.\n",
        "#     callbacks = [EarlyStopping(monitor='val_loss', patience=2)]\n",
        "\n",
        "#     # Train and validate model.\n",
        "#     history = model.fit(\n",
        "#             x_train,\n",
        "#             train[\"sentiment\"],\n",
        "#             epochs=epochs,\n",
        "#             callbacks=callbacks,\n",
        "#             validation_data=(x_val, test[\"sentiment\"]),\n",
        "#             verbose=2,  # Logs once per epoch.\n",
        "#             batch_size=batch_size)\n",
        "\n",
        "#     # Print results.\n",
        "#     history = history.history\n",
        "#     print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
        "#             acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
        "\n",
        "#     # Save model.\n",
        "#     model.save('imdb_mlp_model.h5')\n",
        "#     return history['val_acc'][-1], history['val_loss'][-1]"
      ],
      "metadata": {
        "id": "F5YNy00ieJCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_ngram_model(train, test)"
      ],
      "metadata": {
        "id": "XMYkzjLVZzPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model.get_config()"
      ],
      "metadata": {
        "id": "NSZ6_M9gE74P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.utils import plot_model\n",
        "# plot(model, to_file='tfNN_model.png')"
      ],
      "metadata": {
        "id": "QSMRDfgeE9MZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}