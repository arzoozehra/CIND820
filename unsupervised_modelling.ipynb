{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arzoozehra/CIND820/blob/main/unsupervised_modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import libraries**"
      ],
      "metadata": {
        "id": "-bkY_pYm8WwM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Wr2dHIs7WLWJ",
        "outputId": "02234bef-2f89-474e-9dde-3804400fbb65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "#!pip install contractions\n",
        "import contractions\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "#!pip install pyspellchecker\n",
        "#from spellchecker import SpellChecker\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "#from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from tensorflow.python.keras import models\n",
        "from tensorflow.python.keras.layers import Dense, Dropout\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping\n",
        "from tensorflow.python.keras.losses import SparseCategoricalCrossentropy\n",
        "from tensorflow.python.keras.optimizer_v2.adam import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load data**"
      ],
      "metadata": {
        "id": "GfYAFFPs8gNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/arzoozehra/CIND820/main/data/train.csv\"\n",
        "train = pd.read_csv(url)\n",
        "test = pd.read_csv(\"https://raw.githubusercontent.com/arzoozehra/CIND820/main/data/test.csv\")\n",
        "\n",
        "train.drop([\"textID\", \"selected_text\"], axis=1, inplace=True)\n",
        "test.drop([\"textID\"], axis=1, inplace=True)\n",
        "\n",
        "# Remove row with missing values\n",
        "train.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "WzPmmeuGrev1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clean data**"
      ],
      "metadata": {
        "id": "QbW4UiEpq9kA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(data):\n",
        "  \n",
        "  # Convert text to lowercase\n",
        "  data[\"text\"] = data[\"text\"].str.lower()\n",
        "\n",
        "  # Expand contractions e.g \"gonna\" to \"going to\" and \"i've\" to \"i have\"\n",
        "  data[\"text\"].replace( {r\"`\": \"'\"}, inplace= True, regex = True)\n",
        "  data[\"text\"] = data[\"text\"].apply(contractions.fix)\n",
        "\n",
        "  # Remove @, Unicode characters, punctuation, emojis, URLs, retweets, words with digits, and 1 or 2 letter words\n",
        "  data[\"text\"].replace( {r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?|\\w*\\d\\w*|\\b\\w{1,2}\\b\": \" \"}, inplace= True, regex = True)\n",
        "\n",
        "  # Remove extra whitespaces\n",
        "  data[\"text\"].replace( {r\" +\": \" \"}, inplace= True, regex = True)\n",
        "  data[\"text\"] = data[\"text\"].str.strip()\n",
        "\n",
        "  # Correct spellings\n",
        "  #spell = SpellChecker()\n",
        "\n",
        "  #def correct_spellings(text):\n",
        "  #    corrected_text = []\n",
        "  #    misspelled_words = {}\n",
        "  #    words = text.split()\n",
        "  #    for w in spell.unknown(words):\n",
        "  #        corr = spell.correction(w)\n",
        "  #        if corr:\n",
        "  #            misspelled_words[w] = spell.correction(w) or w\n",
        "  #    corrected_text = [misspelled_words.get(w, w) for w in words]\n",
        "  #    return \" \".join(corrected_text)\n",
        "\n",
        "  #data[\"text\"] = data[\"text\"].apply(lambda x : correct_spellings(x))\n",
        "\n",
        "  # Remove stopwords\n",
        "  stop = stopwords.words(\"english\")\n",
        "  data[\"text\"] = data[\"text\"].apply(lambda text: \" \".join([word for word in text.split() if word not in (stop)]))\n",
        "\n",
        "  # Stemming\n",
        "  stemmer = PorterStemmer()\n",
        "  data[\"text\"] = data[\"text\"].apply(lambda text: \" \".join([stemmer.stem(word) for word in text.split()]))\n",
        "\n",
        "  # Lemmatizing\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  data[\"text\"] = data[\"text\"].apply(lambda text: \" \".join([lemmatizer.lemmatize(word) for word in text.split()]))\n",
        "\n",
        "  return data"
      ],
      "metadata": {
        "id": "5tDnIMd-q89D"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean trraining data\n",
        "train = clean_data(train)\n",
        "\n",
        "#Clean testing data\n",
        "test = clean_data(test)"
      ],
      "metadata": {
        "id": "lil3NvKYD-3j"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Feature Selection**"
      ],
      "metadata": {
        "id": "SRySCY_E8Kke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorization parameters\n",
        "\n",
        "def ngram_vectorize(train_texts, train_labels, val_texts):\n",
        "  \"\"\"Vectorizes texts as ngram vectors.\n",
        "  1 text = 1 tf-idf vector the length of vocabulary of uni-grams + bi-grams.\n",
        "  # Arguments\n",
        "      train_texts: list, training text strings.\n",
        "      train_labels: np.ndarray, training labels.\n",
        "      val_texts: list, validation text strings.\n",
        "  # Returns\n",
        "      x_train, x_val: vectorized training and validation texts\n",
        "  \"\"\"\n",
        "  # Range (inclusive) of n-gram sizes for tokenizing text.\n",
        "  # Use 1-grams + 2-grams.\n",
        "  NGRAM_RANGE = (1, 2)\n",
        "\n",
        "  # Whether text should be split into word or character n-grams.\n",
        "  # Split text into word tokens.\n",
        "  TOKEN_MODE = 'word'\n",
        "\n",
        "  # Minimum document/corpus frequency below which a token will be discarded.\n",
        "  MIN_DOCUMENT_FREQUENCY = 5\n",
        "\n",
        "  # Limit on the number of features. We use the top 20K features.\n",
        "  TOP_K = 20000\n",
        "\n",
        "  # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
        "  kwargs = {\n",
        "          \"ngram_range\": NGRAM_RANGE,\n",
        "          \"analyzer\": TOKEN_MODE,  \n",
        "          \"min_df\": MIN_DOCUMENT_FREQUENCY,\n",
        "          \"max_df\" : 0.8,\n",
        "          \"sublinear_tf\": \"True\"\n",
        "  }\n",
        "  vectorizer = TfidfVectorizer(**kwargs)\n",
        "\n",
        "  # Learn vocabulary from training texts and vectorize training texts.\n",
        "  x_train = vectorizer.fit_transform(train_texts).toarray()\n",
        "\n",
        "  # Vectorize validation texts.\n",
        "  x_val = vectorizer.transform(val_texts).toarray()\n",
        "\n",
        "  # # Select top 'k' of the vectorized features.\n",
        "  # selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
        "  # selector.fit(x_train, train_labels)\n",
        "  # x_train = selector.transform(x_train)\n",
        "  # x_val = selector.transform(x_val)\n",
        "\n",
        "  # x_train = x_train.toarray()\n",
        "  # x_val = x_val.toarray()\n",
        "  return x_train, x_val"
      ],
      "metadata": {
        "id": "eVC51r3aO6-6"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.shape(x_train))\n",
        "print(np.shape(x_val))\n",
        "# print(type(train_vectors))\n",
        "# print(type(test_vectors))\n",
        "# print(train_vectors)\n",
        "# print(test_vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EUPVYY3EF8S",
        "outputId": "f4a18762-f891-407e-928b-94ba12fee16d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(27480, 5713)\n",
            "(3534, 5713)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unsupervised modelling using TensorFlow**"
      ],
      "metadata": {
        "id": "fp5HLVdNboZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp_model(layers, units, dropout_rate, input_shape, op_units=3, op_activation='softmax'):\n",
        "    \"\"\"Creates an instance of a multi-layer perceptron model.\n",
        "\n",
        "    # Arguments\n",
        "        layers: int, number of `Dense` layers in the model.\n",
        "        units: int, output dimension of the layers.\n",
        "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
        "        input_shape: tuple, shape of input to the model.\n",
        "        op_units: int, number of output classes.\n",
        "        op_activation: softmax for multiclass\n",
        "\n",
        "    # Returns\n",
        "        An MLP model instance.\n",
        "    \"\"\"\n",
        "\n",
        "    model = models.Sequential()\n",
        "    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n",
        "\n",
        "    for _ in range(layers-1):\n",
        "        model.add(Dense(units=units, activation='relu'))\n",
        "        model.add(Dropout(rate=dropout_rate))\n",
        "\n",
        "    model.add(Dense(units=op_units, activation=op_activation))\n",
        "    return model"
      ],
      "metadata": {
        "id": "lyYc_8N3bkb7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train model**"
      ],
      "metadata": {
        "id": "0ujbGFwieJlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate=1e-3\n",
        "epochs=1000\n",
        "batch_size=128\n",
        "layers=2\n",
        "units=64\n",
        "dropout_rate=0.2\n",
        "\n",
        "\"\"\"Trains n-gram model on the given dataset.\n",
        "\n",
        "# Arguments\n",
        "    train, test: tuples of training and test texts and labels.\n",
        "    learning_rate: float, learning rate for training model.\n",
        "    epochs: int, number of epochs.\n",
        "    batch_size: int, number of samples per batch.\n",
        "    layers: int, number of `Dense` layers in the model.\n",
        "    units: int, output dimension of Dense layers in the model.\n",
        "    dropout_rate: float: percentage of input to drop at Dropout layers.\n",
        "\"\"\"\n",
        "# Encode train and test labels\n",
        "le = LabelEncoder()\n",
        "train_labels = le.fit_transform(train[\"sentiment\"])\n",
        "test_labels = le.fit_transform(test[\"sentiment\"])\n",
        "\n",
        "# Vectorize texts.\n",
        "x_train, x_val = ngram_vectorize(train[\"text\"], train_labels, test[\"text\"])\n",
        "\n",
        "# Create model instance.\n",
        "model = mlp_model(layers=layers,\n",
        "                  units=units,\n",
        "                  dropout_rate=dropout_rate,\n",
        "                  input_shape=x_train.shape[1:])\n",
        "\n",
        "# Compile model with learning parameters.\n",
        "loss = 'sparse_categorical_crossentropy'\n",
        "optimizer = Adam(learning_rate=learning_rate)\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
        "\n",
        "# Create callback for early stopping on validation loss. If the loss does\n",
        "# not decrease in two consecutive tries, stop training.\n",
        "callbacks = [EarlyStopping(monitor='val_loss', patience=2)]\n",
        "\n",
        "# Train and validate model.\n",
        "history = model.fit(\n",
        "        x_train,\n",
        "        train_labels,\n",
        "        epochs=epochs,\n",
        "        callbacks=callbacks,\n",
        "        validation_data=(x_val, test_labels),\n",
        "        verbose=2,  # Logs once per epoch.\n",
        "        batch_size=batch_size\n",
        ")\n",
        "\n",
        "# Print results.\n",
        "history = history.history\n",
        "print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
        "        acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
        "\n",
        "# Save model.\n",
        "model.save('imdb_mlp_model.h5')\n",
        "print(history['val_acc'][-1], history['val_loss'][-1])"
      ],
      "metadata": {
        "id": "C0ySmmzIdhdS",
        "outputId": "f6a6076e-cc4d-4202-a163-62c69a68e635",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "215/215 - 6s - loss: 0.9643 - acc: 0.5455\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/1000\n",
            "215/215 - 4s - loss: 0.7572 - acc: 0.6857\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/1000\n",
            "215/215 - 5s - loss: 0.6891 - acc: 0.7157\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/1000\n",
            "215/215 - 4s - loss: 0.6589 - acc: 0.7283\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/1000\n",
            "215/215 - 4s - loss: 0.6370 - acc: 0.7379\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/1000\n",
            "215/215 - 4s - loss: 0.6254 - acc: 0.7440\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/1000\n",
            "215/215 - 7s - loss: 0.6041 - acc: 0.7525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/1000\n",
            "215/215 - 4s - loss: 0.5938 - acc: 0.7559\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/1000\n",
            "215/215 - 4s - loss: 0.5834 - acc: 0.7624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,acc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_ngram_model(train, test,\n",
        "#                       learning_rate=1e-3,\n",
        "#                       epochs=100,\n",
        "#                       batch_size=128,\n",
        "#                       layers=2,\n",
        "#                       units=64,\n",
        "#                       dropout_rate=0.2):\n",
        "#     \"\"\"Trains n-gram model on the given dataset.\n",
        "\n",
        "#     # Arguments\n",
        "#         train, test: tuples of training and test texts and labels.\n",
        "#         learning_rate: float, learning rate for training model.\n",
        "#         epochs: int, number of epochs.\n",
        "#         batch_size: int, number of samples per batch.\n",
        "#         layers: int, number of `Dense` layers in the model.\n",
        "#         units: int, output dimension of Dense layers in the model.\n",
        "#         dropout_rate: float: percentage of input to drop at Dropout layers.\n",
        "#     \"\"\"\n",
        "\n",
        "\n",
        "#     #(train_texts, train_labels), (val_texts, val_labels) = data\n",
        "\n",
        "#     # Vectorize texts.\n",
        "#     x_train, x_val = ngram_vectorize(train[\"text\"], train[\"sentiment\"], test[\"text\"])\n",
        "    \n",
        "#     # Create model instance.\n",
        "#     model = mlp_model(layers=layers,\n",
        "#                       units=units,\n",
        "#                       dropout_rate=dropout_rate,\n",
        "#                       input_shape=x_train.shape[1:])\n",
        "\n",
        "#     # Compile model with learning parameters.\n",
        "#     loss = 'sparse_categorical_crossentropy'\n",
        "#     optimizer = Adam(lr=learning_rate)\n",
        "#     model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
        "\n",
        "#     # Create callback for early stopping on validation loss. If the loss does\n",
        "#     # not decrease in two consecutive tries, stop training.\n",
        "#     callbacks = [EarlyStopping(monitor='val_loss', patience=2)]\n",
        "\n",
        "#     # Train and validate model.\n",
        "#     history = model.fit(\n",
        "#             x_train,\n",
        "#             train[\"sentiment\"],\n",
        "#             epochs=epochs,\n",
        "#             callbacks=callbacks,\n",
        "#             validation_data=(x_val, test[\"sentiment\"]),\n",
        "#             verbose=2,  # Logs once per epoch.\n",
        "#             batch_size=batch_size)\n",
        "\n",
        "#     # Print results.\n",
        "#     history = history.history\n",
        "#     print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
        "#             acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
        "\n",
        "#     # Save model.\n",
        "#     model.save('imdb_mlp_model.h5')\n",
        "#     return history['val_acc'][-1], history['val_loss'][-1]"
      ],
      "metadata": {
        "id": "F5YNy00ieJCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_ngram_model(train, test)"
      ],
      "metadata": {
        "id": "XMYkzjLVZzPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model.get_config()"
      ],
      "metadata": {
        "id": "NSZ6_M9gE74P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.utils import plot_model\n",
        "# plot(model, to_file='tfNN_model.png')"
      ],
      "metadata": {
        "id": "QSMRDfgeE9MZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}