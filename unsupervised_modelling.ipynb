{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arzoozehra/CIND820/blob/main/unsupervised_modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import libraries**"
      ],
      "metadata": {
        "id": "-bkY_pYm8WwM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Wr2dHIs7WLWJ",
        "outputId": "db3c8bbc-70e8-4988-a297-9ed2456b2757",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.8/dist-packages (0.1.73)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.8/dist-packages (from contractions) (0.0.24)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.8/dist-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.8/dist-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "!pip install contractions\n",
        "import contractions\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "#!pip install pyspellchecker\n",
        "#from spellchecker import SpellChecker\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "#from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from tensorflow.python.keras import models\n",
        "from tensorflow.python.keras.layers import Dense, Dropout\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping\n",
        "from tensorflow.python.keras.optimizer_v2.adam import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load data**"
      ],
      "metadata": {
        "id": "GfYAFFPs8gNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/arzoozehra/CIND820/main/data/train.csv\"\n",
        "train = pd.read_csv(url)\n",
        "test = pd.read_csv(\"https://raw.githubusercontent.com/arzoozehra/CIND820/main/data/test.csv\")\n",
        "\n",
        "train.drop([\"textID\", \"selected_text\"], axis=1, inplace=True)\n",
        "test.drop([\"textID\"], axis=1, inplace=True)\n",
        "\n",
        "# Remove row with missing values\n",
        "train.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "WzPmmeuGrev1"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clean data**"
      ],
      "metadata": {
        "id": "QbW4UiEpq9kA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(data):\n",
        "  \n",
        "  # Convert text to lowercase\n",
        "  data[\"text\"] = data[\"text\"].str.lower()\n",
        "\n",
        "  # Expand contractions e.g \"gonna\" to \"going to\" and \"i've\" to \"i have\"\n",
        "  data[\"text\"].replace( {r\"`\": \"'\"}, inplace= True, regex = True)\n",
        "  data[\"text\"] = data[\"text\"].apply(contractions.fix)\n",
        "\n",
        "  # Remove @, Unicode characters, punctuation, emojis, URLs, retweets, words with digits, and 1 or 2 letter words\n",
        "  data[\"text\"].replace( {r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?|\\w*\\d\\w*|\\b\\w{1,2}\\b\": \" \"}, inplace= True, regex = True)\n",
        "\n",
        "  # Remove extra whitespaces\n",
        "  data[\"text\"].replace( {r\" +\": \" \"}, inplace= True, regex = True)\n",
        "  data[\"text\"] = data[\"text\"].str.strip()\n",
        "\n",
        "  # Correct spellings\n",
        "  #spell = SpellChecker()\n",
        "\n",
        "  #def correct_spellings(text):\n",
        "  #    corrected_text = []\n",
        "  #    misspelled_words = {}\n",
        "  #    words = text.split()\n",
        "  #    for w in spell.unknown(words):\n",
        "  #        corr = spell.correction(w)\n",
        "  #        if corr:\n",
        "  #            misspelled_words[w] = spell.correction(w) or w\n",
        "  #    corrected_text = [misspelled_words.get(w, w) for w in words]\n",
        "  #    return \" \".join(corrected_text)\n",
        "\n",
        "  #data[\"text\"] = data[\"text\"].apply(lambda x : correct_spellings(x))\n",
        "\n",
        "  # Remove stopwords\n",
        "  stop = stopwords.words(\"english\")\n",
        "  data[\"text\"] = data[\"text\"].apply(lambda text: \" \".join([word for word in text.split() if word not in (stop)]))\n",
        "\n",
        "  # Stemming\n",
        "  stemmer = PorterStemmer()\n",
        "  data[\"text\"] = data[\"text\"].apply(lambda text: \" \".join([stemmer.stem(word) for word in text.split()]))\n",
        "\n",
        "  # Lemmatizing\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  data[\"text\"] = data[\"text\"].apply(lambda text: \" \".join([lemmatizer.lemmatize(word) for word in text.split()]))\n",
        "\n",
        "  return data"
      ],
      "metadata": {
        "id": "5tDnIMd-q89D"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean trraining data\n",
        "train = clean_data(train)\n",
        "\n",
        "#Clean testing data\n",
        "test = clean_data(test)"
      ],
      "metadata": {
        "id": "lil3NvKYD-3j"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Splitting training data into training and validation set**"
      ],
      "metadata": {
        "id": "RmKgQz_3HuDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_training_and_validation_sets(texts, labels, validation_split):\n",
        "  \"\"\"Splits the texts and labels into training and validation sets.\n",
        "  # Arguments\n",
        "      texts: list, text data.\n",
        "      labels: list, label data.\n",
        "      validation_split: float, percentage of data to use for validation.\n",
        "  # Returns\n",
        "      A tuple of training and validation data.\n",
        "  \"\"\"\n",
        "  num_training_samples = int((1 - validation_split) * len(texts))\n",
        "  return ((texts[:num_training_samples], labels[:num_training_samples]),\n",
        "          (texts[num_training_samples:], labels[num_training_samples:]))"
      ],
      "metadata": {
        "id": "dYbOkr7gLKou"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Feature Selection**"
      ],
      "metadata": {
        "id": "SRySCY_E8Kke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorization parameters\n",
        "\n",
        "def ngram_vectorize(train_texts, train_labels, val_texts):\n",
        "  \"\"\"Vectorizes texts as ngram vectors.\n",
        "  1 text = 1 tf-idf vector the length of vocabulary of uni-grams + bi-grams.\n",
        "  # Arguments\n",
        "      train_texts: list, training text strings.\n",
        "      train_labels: np.ndarray, training labels.\n",
        "      val_texts: list, validation text strings.\n",
        "  # Returns\n",
        "      x_train, x_val: vectorized training and validation texts\n",
        "  \"\"\"\n",
        "  # Range (inclusive) of n-gram sizes for tokenizing text.\n",
        "  # Use 1-grams + 2-grams.\n",
        "  NGRAM_RANGE = (1, 2)\n",
        "\n",
        "  # Whether text should be split into word or character n-grams.\n",
        "  # Split text into word tokens.\n",
        "  TOKEN_MODE = 'word'\n",
        "\n",
        "  # Minimum document/corpus frequency below which a token will be discarded.\n",
        "  MIN_DOCUMENT_FREQUENCY = 5\n",
        "\n",
        "  # Limit on the number of features. We use the top 20K features.\n",
        "  TOP_K = 20000\n",
        "\n",
        "  # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
        "  kwargs = {\n",
        "          \"ngram_range\": NGRAM_RANGE,\n",
        "          \"dtype\" : \"int32\",\n",
        "          \"analyzer\": TOKEN_MODE,  \n",
        "          \"min_df\": MIN_DOCUMENT_FREQUENCY,\n",
        "          \"max_df\" : 0.8,\n",
        "          \"sublinear_tf\": \"True\"\n",
        "  }\n",
        "  vectorizer = TfidfVectorizer(**kwargs)\n",
        "\n",
        "  # Learn vocabulary from training texts and vectorize training texts.\n",
        "  x_train = vectorizer.fit_transform(train_texts)\n",
        "\n",
        "  # Vectorize validation texts.\n",
        "  x_val = vectorizer.transform(val_texts)\n",
        "\n",
        "  # Select top 'k' of the vectorized features.\n",
        "  selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
        "  selector.fit(x_train, train_labels)\n",
        "  x_train = selector.transform(x_train)\n",
        "  x_val = selector.transform(x_val)\n",
        "\n",
        "  x_train = x_train.astype('float32')\n",
        "  x_val = x_val.astype('float32')\n",
        "  return x_train, x_val"
      ],
      "metadata": {
        "id": "eVC51r3aO6-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(np.shape(train_vectors))\n",
        "# print(np.shape(test_vectors))\n",
        "# print(type(train_vectors))\n",
        "# print(type(test_vectors))\n",
        "# print(train_vectors)\n",
        "# print(test_vectors)"
      ],
      "metadata": {
        "id": "2EUPVYY3EF8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unsupervised modelling using TensorFlow**"
      ],
      "metadata": {
        "id": "fp5HLVdNboZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp_model(layers, units, dropout_rate, input_shape, op_units=3, op_activation='softmax'):\n",
        "    \"\"\"Creates an instance of a multi-layer perceptron model.\n",
        "\n",
        "    # Arguments\n",
        "        layers: int, number of `Dense` layers in the model.\n",
        "        units: int, output dimension of the layers.\n",
        "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
        "        input_shape: tuple, shape of input to the model.\n",
        "        op_units: int, number of output classes.\n",
        "        op_activation: softmax for multiclass\n",
        "\n",
        "    # Returns\n",
        "        An MLP model instance.\n",
        "    \"\"\"\n",
        "\n",
        "    model = models.Sequential()\n",
        "    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n",
        "\n",
        "    for _ in range(layers-1):\n",
        "        model.add(Dense(units=units, activation='relu'))\n",
        "        model.add(Dropout(rate=dropout_rate))\n",
        "\n",
        "    model.add(Dense(units=op_units, activation=op_activation))\n",
        "    return model"
      ],
      "metadata": {
        "id": "lyYc_8N3bkb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train model**"
      ],
      "metadata": {
        "id": "0ujbGFwieJlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ngram_model(train, test,\n",
        "                      learning_rate=1e-3,\n",
        "                      epochs=1000,\n",
        "                      batch_size=128,\n",
        "                      layers=2,\n",
        "                      units=64,\n",
        "                      dropout_rate=0.2):\n",
        "    \"\"\"Trains n-gram model on the given dataset.\n",
        "\n",
        "    # Arguments\n",
        "        train, test: tuples of training and test texts and labels.\n",
        "        learning_rate: float, learning rate for training model.\n",
        "        epochs: int, number of epochs.\n",
        "        batch_size: int, number of samples per batch.\n",
        "        layers: int, number of `Dense` layers in the model.\n",
        "        units: int, output dimension of Dense layers in the model.\n",
        "        dropout_rate: float: percentage of input to drop at Dropout layers.\n",
        "    \"\"\"\n",
        "\n",
        "    # Vectorize texts.\n",
        "    x_train, x_val = ngram_vectorize(train[\"text\"], train[\"sentiment\"], test[\"text\"])\n",
        "    \n",
        "    # Create model instance.\n",
        "    model = mlp_model(layers=layers,\n",
        "                      units=units,\n",
        "                      dropout_rate=dropout_rate,\n",
        "                      input_shape=x_train.shape[1:])\n",
        "\n",
        "    # Compile model with learning parameters.\n",
        "    loss = 'sparse_categorical_crossentropy'\n",
        "    optimizer = Adam(lr=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
        "\n",
        "    # Create callback for early stopping on validation loss. If the loss does\n",
        "    # not decrease in two consecutive tries, stop training.\n",
        "    callbacks = [EarlyStopping(monitor='val_loss', patience=2)]\n",
        "\n",
        "    # Train and validate model.\n",
        "    history = model.fit(\n",
        "            x_train,\n",
        "            train_labels,\n",
        "            epochs=epochs,\n",
        "            callbacks=callbacks,\n",
        "            validation_data=(x_val, val_labels),\n",
        "            verbose=2,  # Logs once per epoch.\n",
        "            batch_size=batch_size)\n",
        "\n",
        "    # Print results.\n",
        "    history = history.history\n",
        "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
        "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
        "\n",
        "    # Save model.\n",
        "    model.save('imdb_mlp_model.h5')\n",
        "    return history['val_acc'][-1], history['val_loss'][-1]"
      ],
      "metadata": {
        "id": "F5YNy00ieJCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ngram_model(train)"
      ],
      "metadata": {
        "id": "XMYkzjLVZzPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_config()"
      ],
      "metadata": {
        "id": "NSZ6_M9gE74P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "plot(model, to_file='tfNN_model.png')"
      ],
      "metadata": {
        "id": "QSMRDfgeE9MZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}