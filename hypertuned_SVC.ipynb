{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arzoozehra/CIND820/blob/main/hypertuned_SVC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import libraries**"
      ],
      "metadata": {
        "id": "-bkY_pYm8WwM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wr2dHIs7WLWJ",
        "outputId": "d814cea3-22bc-4336-d517-e927629d86d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii\n",
            "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 3.8 MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (110 kB)\n",
            "\u001b[K     |████████████████████████████████| 110 kB 8.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.1 contractions-0.1.73 pyahocorasick-1.4.4 textsearch-0.0.24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "!pip install contractions\n",
        "import contractions\n",
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "#!pip install pyspellchecker\n",
        "#from spellchecker import SpellChecker\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.model_selection import cross_val_score, RandomizedSearchCV #, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay, mean_squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load data**"
      ],
      "metadata": {
        "id": "GfYAFFPs8gNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/arzoozehra/CIND820/main/data/train.csv\"\n",
        "train = pd.read_csv(url)\n",
        "test = pd.read_csv(\"https://raw.githubusercontent.com/arzoozehra/CIND820/main/data/test.csv\")\n",
        "\n",
        "train.drop([\"textID\", \"selected_text\"], axis=1, inplace=True)\n",
        "test.drop([\"textID\"], axis=1, inplace=True)\n",
        "\n",
        "# Remove row with missing values\n",
        "train.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "WzPmmeuGrev1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train[\"text\"].head(10))\n",
        "print(train[\"text\"].tail(10))"
      ],
      "metadata": {
        "id": "rQwxYsauWPT1",
        "outputId": "77752971-d469-4f0e-aada-0ab5f4658be5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                  I`d have responded, if I were going\n",
            "1        Sooo SAD I will miss you here in San Diego!!!\n",
            "2                            my boss is bullying me...\n",
            "3                       what interview! leave me alone\n",
            "4     Sons of ****, why couldn`t they put them on t...\n",
            "5    http://www.dothebouncy.com/smf - some shameles...\n",
            "6    2am feedings for the baby are fun when he is a...\n",
            "7                                           Soooo high\n",
            "8                                          Both of you\n",
            "9     Journey!? Wow... u just became cooler.  hehe....\n",
            "Name: text, dtype: object\n",
            "27471    i`m defying gravity. and nobody in alll of oz,...\n",
            "27472    http://twitpic.com/663vr - Wanted to visit the...\n",
            "27473     in spoke to you yesterday and u didnt respond...\n",
            "27474    So I get up early and I feel good about the da...\n",
            "27475                                       enjoy ur night\n",
            "27476     wish we could come see u on Denver  husband l...\n",
            "27477     I`ve wondered about rake to.  The client has ...\n",
            "27478     Yay good for both of you. Enjoy the break - y...\n",
            "27479                           But it was worth it  ****.\n",
            "27480       All this flirting going on - The ATG smiles...\n",
            "Name: text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clean data**"
      ],
      "metadata": {
        "id": "QbW4UiEpq9kA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_data(data):\n",
        "  \n",
        "  # Convert text to lowercase\n",
        "  data[\"text\"] = data[\"text\"].str.lower()\n",
        "\n",
        "  # Expand contractions e.g \"gonna\" to \"going to\" and \"i've\" to \"i have\"\n",
        "  data[\"text\"].replace( {r\"`\": \"'\"}, inplace= True, regex = True)\n",
        "  data[\"text\"] = data[\"text\"].apply(contractions.fix)\n",
        "\n",
        "  # Remove @, Unicode characters, punctuation, emojis, URLs, retweets, words with digits, and 1 or 2 letter words\n",
        "  data[\"text\"].replace( {r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?|\\w*\\d\\w*|\\b\\w{1,2}\\b\": \" \"}, inplace= True, regex = True)\n",
        "\n",
        "  # Remove extra whitespaces\n",
        "  data[\"text\"].replace( {r\" +\": \" \"}, inplace= True, regex = True)\n",
        "  data[\"text\"] = data[\"text\"].str.strip()\n",
        "\n",
        "  # Correct spellings\n",
        "  #spell = SpellChecker()\n",
        "\n",
        "  #def correct_spellings(text):\n",
        "  #    corrected_text = []\n",
        "  #    misspelled_words = {}\n",
        "  #    words = text.split()\n",
        "  #    for w in spell.unknown(words):\n",
        "  #        corr = spell.correction(w)\n",
        "  #        if corr:\n",
        "  #            misspelled_words[w] = spell.correction(w) or w\n",
        "  #    corrected_text = [misspelled_words.get(w, w) for w in words]\n",
        "  #    return \" \".join(corrected_text)\n",
        "\n",
        "  #data[\"text\"] = data[\"text\"].apply(lambda x : correct_spellings(x))\n",
        "\n",
        "  # Remove stopwords\n",
        "  stop = stopwords.words(\"english\")\n",
        "  data[\"text\"] = data[\"text\"].apply(lambda text: \" \".join([word for word in text.split() if word not in (stop)]))\n",
        "\n",
        "  # Stemming\n",
        "  stemmer = PorterStemmer()\n",
        "  data[\"text\"] = data[\"text\"].apply(lambda text: \" \".join([stemmer.stem(word) for word in text.split()]))\n",
        "\n",
        "  # Lemmatizing\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  data[\"text\"] = data[\"text\"].apply(lambda text: \" \".join([lemmatizer.lemmatize(word) for word in text.split()]))\n",
        "\n",
        "  return data"
      ],
      "metadata": {
        "id": "NyWCTuFDeTPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Clean trraining data\n",
        "train = clean_data(train)\n",
        "\n",
        "#Clean testing data\n",
        "test = clean_data(test)"
      ],
      "metadata": {
        "id": "5tDnIMd-q89D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train[\"text\"].head(10))\n",
        "print(train[\"text\"].tail(10))"
      ],
      "metadata": {
        "id": "tT-89PPBe703",
        "outputId": "220addb4-2cfb-489e-f581-196c4e2ace1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                          would respond go\n",
            "1                   sooo sad miss san diego\n",
            "2                                 bos bulli\n",
            "3                       interview leav alon\n",
            "4       son could put releas alreadi bought\n",
            "5    shameless plug best ranger forum earth\n",
            "6                   feed babi fun smile coo\n",
            "7                                soooo high\n",
            "8                                          \n",
            "9     journey wow becam cooler hehe possibl\n",
            "Name: text, dtype: object\n",
            "27471        defi graviti nobodi alll wizard ever go bring\n",
            "27472                                 want visit anim late\n",
            "27473           spoke yesterday respond girl wassup though\n",
            "27474    get earli feel good day walk work feel alright...\n",
            "27475                                          enjoy night\n",
            "27476    wish could come see denver husband lost job ca...\n",
            "27477    wonder rake client made clear net forc dev lea...\n",
            "27478    yay good enjoy break probabl need hectic weeke...\n",
            "27479                                                worth\n",
            "27480                           flirt go atg smile yay hug\n",
            "Name: text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test[\"text\"].head(20))\n",
        "print(test[\"text\"].tail(20))"
      ],
      "metadata": {
        "id": "1C8JW4YmUcbO",
        "outputId": "a0d33b7f-f1f4-41c6-9357-e275c75a8b0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                                      last session day\n",
            "1     shanghai also realli excit precis skyscrap gal...\n",
            "2     recess hit veroniqu branquinho quit compani shame\n",
            "3                                        happi birthday\n",
            "4                                                  like\n",
            "5                                    great weee visitor\n",
            "6                                think everyon hate lol\n",
            "7        soooooo wish could school myspac complet block\n",
            "8                           within short time last clue\n",
            "9     get day alright done anyth yet leav soon steps...\n",
            "10                bike put hold known argh total bummer\n",
            "11                                            check win\n",
            "12                             twitter tavern bore much\n",
            "13    weekend youngest son turn tomorrow make kind s...\n",
            "14          come socket feel like phone hole virgin loo\n",
            "15               hot today like hate new timet bad week\n",
            "16                                                 miss\n",
            "17                                                cramp\n",
            "18          guy say answer question yesterday nice song\n",
            "19          go spiritu stagnent explod ego realis great\n",
            "Name: text, dtype: object\n",
            "3514                                        notic ridicul\n",
            "3515                        ride high low mood chore blow\n",
            "3516            hate websit say ticket price anoth websit\n",
            "3517                                          brake wrrkk\n",
            "3518                     outta follow talk lol shi biteee\n",
            "3519    ye thank haha field flower exist singapor well...\n",
            "3520                                song site want comput\n",
            "3521                      munchin bacon butti woohoo fave\n",
            "3522           school today teacher cancel lesson chillin\n",
            "3523    eye start hurt late must reach updat due tweet...\n",
            "3524    hair dresser pas away yesterday breast cancer ...\n",
            "3525    suppos tomorrow tooooo use ga ticket money pay...\n",
            "3526               best thing ever done carri birth child\n",
            "3527    mother citi terrel texa district citi council ...\n",
            "3528                                     friday even idea\n",
            "3529                                tire cannot sleep tri\n",
            "3530    alon old hous thank net keep aliv kick whoever...\n",
            "3531    know mean littl dog sink depress want move som...\n",
            "3532                sutra next youtub video go love video\n",
            "3533                                  omgssh ang cute bbi\n",
            "Name: text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Feature Selection**"
      ],
      "metadata": {
        "id": "SRySCY_E8Kke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorization parameters\n",
        "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
        "NGRAM_RANGE = (1, 2)  # Use 1-grams + 2-grams.\n",
        "\n",
        "# Whether text should be split into word or character n-grams.\n",
        "TOKEN_MODE = \"word\" # Split text into word tokens.\n",
        "\n",
        "# Minimum document frequency below which a token will be discarded.\n",
        "MIN_DOCUMENT_FREQUENCY = 5\n",
        "\n",
        "# In case of large number of features, limit the number to top 20K features\n",
        "# TOP_K = 20000\n",
        "\n",
        "\n",
        "def ngram_vectorize(train_texts, train_labels, test_texts):\n",
        "    \"\"\"Vectorizes texts as n-gram vectors.\n",
        "\n",
        "    1 text = 1 tf-idf vector the length of vocabulary of unigrams + bigrams.\n",
        "\n",
        "    # Arguments\n",
        "        train_texts: list, training text strings.\n",
        "        train_labels: np.ndarray, training labels.\n",
        "        test_texts: list, test text strings.\n",
        "\n",
        "    # Returns\n",
        "        train_vectors, test_vectors: vectorized training and test texts\n",
        "    \"\"\"\n",
        "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
        "    kwargs = {\n",
        "            \"ngram_range\": NGRAM_RANGE,\n",
        "            \"analyzer\": TOKEN_MODE,  \n",
        "            \"min_df\": MIN_DOCUMENT_FREQUENCY,\n",
        "            \"max_df\" : 0.8,\n",
        "            \"sublinear_tf\": \"True\"\n",
        "    }\n",
        "    vectorizer = TfidfVectorizer(**kwargs)\n",
        "\n",
        "    # Learn vocabulary from training texts and vectorize training texts.\n",
        "    train_vectors = vectorizer.fit_transform(train_texts)\n",
        "\n",
        "    # Vectorize validation texts.\n",
        "    test_vectors = vectorizer.transform(test_texts)\n",
        "\n",
        "    # # Select top 'k' of the vectorized features.\n",
        "    # selector = SelectKBest(f_classif, k=min(TOP_K, train_vectors.shape[1]))\n",
        "    # selector.fit(train_vectors, train_labels)\n",
        "    # train_vectors = selector.transform(train_vectors).astype('float32').toarray()\n",
        "    # test_vectors = selector.transform(test_vectors).astype('float32').toarray()\n",
        "    \n",
        "    return train_vectors, test_vectors"
      ],
      "metadata": {
        "id": "I3KHg2k7kGyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Create feature vectors\n",
        "# vectorizer = TfidfVectorizer(ngram_range = NGRAM_RANGE,\n",
        "#                              min_df = 5,\n",
        "#                              max_df = 0.8,\n",
        "#                              sublinear_tf = True,\n",
        "#                              use_idf = True)\n",
        "# train_vectors = vectorizer.fit_transform(train[\"text\"])\n",
        "# test_vectors = vectorizer.transform(test[\"text\"])\n",
        "\n",
        "\n",
        "train_vectors, test_vectors = ngram_vectorize(train[\"text\"], train[\"sentiment\"], test[\"text\"])"
      ],
      "metadata": {
        "id": "JxxnENgQbT8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.shape(train_vectors))\n",
        "print(np.shape(test_vectors))\n",
        "print(type(train_vectors))\n",
        "print(type(test_vectors))\n",
        "print(train_vectors)\n",
        "print(test_vectors)"
      ],
      "metadata": {
        "id": "0wVyvU22-nrz",
        "outputId": "445ce464-d6fb-44cd-8195-541364427ba2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(27480, 5713)\n",
            "(3534, 5713)\n",
            "<class 'scipy.sparse.csr.csr_matrix'>\n",
            "<class 'scipy.sparse.csr.csr_matrix'>\n",
            "  (0, 1820)\t6.680787890633085\n",
            "  (0, 4000)\t50.435061059379805\n",
            "  (0, 5589)\t10.467350005617218\n",
            "  (1, 4105)\t63.941150464501376\n",
            "  (1, 4082)\t43.028521878271285\n",
            "  (1, 1162)\t50.691809377329804\n",
            "  (1, 4104)\t35.435242361326395\n",
            "  (1, 3109)\t3.3937028216932177\n",
            "  (1, 4079)\t5.46516336457297\n",
            "  (1, 4436)\t12.482353018820605\n",
            "  (2, 480)\t67.76129894449726\n",
            "  (3, 2682)\t74.3947639326752\n",
            "  (3, 96)\t18.535719332325943\n",
            "  (3, 2681)\t10.82169860420464\n",
            "  (3, 2430)\t32.145644940742706\n",
            "  (4, 486)\t21.191549155831424\n",
            "  (4, 99)\t10.635892558151976\n",
            "  (4, 3977)\t31.257569926321395\n",
            "  (4, 3843)\t13.855875426055698\n",
            "  (4, 897)\t8.14098204258858\n",
            "  (4, 4421)\t24.715339898954866\n",
            "  (5, 1277)\t31.226373107552185\n",
            "  (5, 1625)\t43.552507910811364\n",
            "  (5, 3881)\t73.62357666319974\n",
            "  (5, 384)\t8.850682711157091\n",
            "  :\t:\n",
            "  (27476, 2628)\t37.88449599171214\n",
            "  (27476, 2677)\t11.501500691375675\n",
            "  (27476, 2943)\t7.718292361129204\n",
            "  (27476, 1611)\t28.84084484488199\n",
            "  (27476, 5518)\t8.757188321986597\n",
            "  (27476, 3336)\t4.59004741785281\n",
            "  (27477, 5651)\t60.60936294026366\n",
            "  (27477, 5642)\t33.29546088915808\n",
            "  (27477, 2188)\t45.07987514024796\n",
            "  (27477, 4723)\t28.20816930713074\n",
            "  (27477, 5394)\t5.117046913809042\n",
            "  (27477, 506)\t10.425903169736642\n",
            "  (27477, 3282)\t3.996150586378292\n",
            "  (27477, 2356)\t16.565153266954024\n",
            "  (27477, 1330)\t7.072163995508071\n",
            "  (27477, 3798)\t9.706240106170874\n",
            "  (27477, 4721)\t5.620585978160358\n",
            "  (27477, 5648)\t6.565132280315933\n",
            "  (27477, 1887)\t2.5465211212192758\n",
            "  (27477, 646)\t13.270053227273142\n",
            "  (27478, 5587)\t49.02137479992611\n",
            "  (27479, 2345)\t19.32061757356005\n",
            "  (27479, 5648)\t14.176454059118107\n",
            "  (27479, 4369)\t27.406024444644284\n",
            "  (27479, 1820)\t5.695492726577126\n",
            "  (0, 4232)\t49.524164097905164\n",
            "  (0, 2634)\t12.737270971116043\n",
            "  (0, 1021)\t5.608138019514453\n",
            "  (1, 5129)\t38.66573203010497\n",
            "  (1, 3916)\t70.77078335488228\n",
            "  (1, 3907)\t5.116957200274288\n",
            "  (1, 1887)\t3.60114293335642\n",
            "  (1, 1410)\t10.348212409449962\n",
            "  (1, 730)\t53.90433555803965\n",
            "  (1, 104)\t11.463892400787772\n",
            "  (2, 4242)\t28.212690162931928\n",
            "  (2, 3856)\t19.623763565782546\n",
            "  (2, 2228)\t19.51714996488483\n",
            "  (2, 838)\t30.504825247263497\n",
            "  (3, 2123)\t29.172055860695007\n",
            "  (3, 2122)\t7.980627713016364\n",
            "  (3, 418)\t17.291696604432147\n",
            "  (4, 2725)\t17.928762615055227\n",
            "  (5, 2011)\t22.393791552162085\n",
            "  (6, 4852)\t8.987353343943743\n",
            "  (6, 2816)\t7.698754662280724\n",
            "  (6, 2150)\t11.759002488963068\n",
            "  (6, 1384)\t17.044456099687213\n",
            "  (7, 5490)\t13.712713479678204\n",
            "  (7, 5488)\t5.077687031035388\n",
            "  :\t:\n",
            "  (3530, 2432)\t42.01329703510906\n",
            "  (3530, 2337)\t6.576662150528999\n",
            "  (3530, 2099)\t8.22183577138451\n",
            "  (3530, 96)\t10.116925507123844\n",
            "  (3530, 85)\t30.241262897811566\n",
            "  (3531, 5309)\t50.33827153805267\n",
            "  (3531, 5292)\t3.204047390157587\n",
            "  (3531, 4307)\t60.004666653046556\n",
            "  (3531, 3209)\t9.82028560711432\n",
            "  (3531, 3039)\t7.643347989962211\n",
            "  (3531, 2790)\t8.045216955196564\n",
            "  (3531, 2589)\t37.623614429192564\n",
            "  (3531, 2571)\t3.44229653932125\n",
            "  (3531, 1202)\t11.049529074910561\n",
            "  (3531, 1140)\t15.719443670044845\n",
            "  (3532, 5699)\t14.84905757898508\n",
            "  (3532, 5248)\t21.751470175347475\n",
            "  (3532, 3369)\t7.9504143519080746\n",
            "  (3532, 2915)\t76.10154892543292\n",
            "  (3532, 2882)\t3.1631226899471634\n",
            "  (3532, 1846)\t69.24418598208946\n",
            "  (3532, 1820)\t3.39435386090943\n",
            "  (3533, 983)\t13.773147391087518\n",
            "  (3533, 334)\t85.07448648613608\n",
            "  (3533, 134)\t54.25202670596041\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Standardization**"
      ],
      "metadata": {
        "id": "3_4Uuxaq-SOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler(with_mean = False)\n",
        "train_vectors = scaler.fit_transform(train_vectors)\n",
        "test_vectors = scaler.transform(test_vectors)"
      ],
      "metadata": {
        "id": "EOH3YMGo-Vwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SVC (linear kernel) modelling and evaluation**"
      ],
      "metadata": {
        "id": "qSC6vhwKaAb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SVC(kernel=\"linear\", random_state=42)\n",
        "model_name = \"SVC (linear kernel)\"\n",
        "\n",
        "# 5-fold Cross-validation\n",
        "k = 5\n",
        "cv_df = pd.DataFrame(index=range(k))\n",
        "entries = []\n",
        "\n",
        "accuracies = cross_val_score(model, train_vectors, train[\"sentiment\"], scoring=\"accuracy\", cv=k)\n",
        "for fold_id, accuracy in enumerate(accuracies):\n",
        "  entries.append((fold_id, accuracy))\n",
        "\n",
        "cv_df = pd.DataFrame(entries, columns=[\"fold_id\", \"accuracy\"])"
      ],
      "metadata": {
        "id": "wfQ4nX-DdPtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_accuracy = cv_df.accuracy.mean()\n",
        "std_accuracy = cv_df.accuracy.std()\n",
        "\n",
        "acc = pd.DataFrame([[mean_accuracy,std_accuracy]], columns = [\"Accuracy\", \" Std dev\"])\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "TE3JzHbFR2lC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_vectors, train[\"sentiment\"])\n",
        "prediction = model.predict(test_vectors)\n",
        "print(f\"Test set accuracy: {accuracy_score(test[\"sentiment\"], prediction) * 100} %\\n\")"
      ],
      "metadata": {
        "id": "8TXDJnAAjs3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report\n",
        "print(f\"\\tCLASSIFICATIION METRICS - {model_name}\\n\")\n",
        "print(classification_report(test[\"sentiment\"], prediction, target_names= [\"negative\", \"neutral\", \"positive\"]))"
      ],
      "metadata": {
        "id": "IsfG8KcKj3g0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = confusion_matrix(test[\"sentiment\"], prediction)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=data, display_labels=model.classes_)\n",
        "disp.plot(cmap=\"Blues\")\n",
        "plt.ylabel(\"ACTUAL\")\n",
        "plt.xlabel(\"\\nPREDICTED\")\n",
        "plt.title(\"\\nCONFUSION MATRIX - SVC (linear kernel)\\n\");\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LXYBbBiHkFtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypertuning the model parameters**"
      ],
      "metadata": {
        "id": "i_8q07ADycg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\"kernel\": (\"linear\", \"rbf\",\"poly\") , \n",
        "              \"C\":[5, 10, 90],\n",
        "              \"gamma\": [1,0.1,0.01,0.001], \n",
        "              \"degree\": [2,3,4,5,6]}"
      ],
      "metadata": {
        "id": "eBGX65Zvsoff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grid = GridSearchCV(SVC() , param_grid , refit=True , verbose=3)\n",
        "# grid.fit(train_vectors, train[\"sentiment\"])"
      ],
      "metadata": {
        "id": "zF1kWe7YtTJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_search = RandomizedSearchCV(SVC(), param_grid, refit = True, verbose = 3, cv = 3)\n",
        "random_search.fit(train_vectors, train[\"sentiment\"])"
      ],
      "metadata": {
        "id": "v5OaE5rJspWM",
        "outputId": "739f60d6-b2dc-4dda-b8d0-38d1e800cf27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(random_search.best_params_)"
      ],
      "metadata": {
        "id": "WlWY9R2zuSJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(random_search.best_estimator_)"
      ],
      "metadata": {
        "id": "Y3y8TdviuTqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(random_search.best_score_)"
      ],
      "metadata": {
        "id": "h53_h0A74Jba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluating the model with best parameters**"
      ],
      "metadata": {
        "id": "c2AmW1a5yw2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_predictions = random_search.best_estimator_.predict(train_vectors)\n",
        "accuracy_score(train[\"sentiment\"], random_predictions)"
      ],
      "metadata": {
        "id": "riLXlVEGuYSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse = mean_squared_error(train[\"sentiment\"], random_predictions)\n",
        "np.sqrt(mse)"
      ],
      "metadata": {
        "id": "mPP-XW3_5N4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_predictions = random_search.best_estimator_.predict(test_vectors)\n",
        "accuracy_score(test[\"sentiment\"], random_predictions)"
      ],
      "metadata": {
        "id": "AlNEJ3SM4n0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse = mean_squared_error(test[\"sentiment\"], random_predictions)\n",
        "np.sqrt(mse)"
      ],
      "metadata": {
        "id": "3BYx0DdV5Pxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(test[\"sentiment\"], random_predictions))"
      ],
      "metadata": {
        "id": "M6JgIYiruvHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(test[\"sentiment\"], random_predictions))"
      ],
      "metadata": {
        "id": "CYt8fTJfyMaD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}