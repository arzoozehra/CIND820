{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arzoozehra/CIND820/blob/main/xgboost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import libraries**"
      ],
      "metadata": {
        "id": "-bkY_pYm8WwM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Wr2dHIs7WLWJ",
        "outputId": "9ed9ef8a-44e8-4cb5-c8d0-9997bab53e5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import time\n",
        "#!pip install contractions\n",
        "import contractions\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "#!pip install pyspellchecker\n",
        "#from spellchecker import SpellChecker\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from collections import Counter\n",
        "import plotly.express as px\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from xgboost import XGBClassifier\n",
        "from tensorflow.python.keras import models\n",
        "from tensorflow.python.keras.layers import Dense, Dropout\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping\n",
        "from tensorflow.python.keras.optimizer_v2.adam import Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "16mKpqz_p2z7"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load data**"
      ],
      "metadata": {
        "id": "GfYAFFPs8gNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://raw.githubusercontent.com/arzoozehra/CIND820/main/data/train.csv'\n",
        "train = pd.read_csv(url)\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/arzoozehra/CIND820/main/data/test.csv')"
      ],
      "metadata": {
        "id": "WzPmmeuGrev1"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explore data**"
      ],
      "metadata": {
        "id": "nVV3p4fZ8msc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove row with missing values\n",
        "train.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "rWNi6b_FuON6"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHQUbYgjuTdw",
        "outputId": "6d772f29-835d-4e00-afb3-9fc42774008b"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 27480 entries, 0 to 27480\n",
            "Data columns (total 4 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   textID         27480 non-null  object\n",
            " 1   text           27480 non-null  object\n",
            " 2   selected_text  27480 non-null  object\n",
            " 3   sentiment      27480 non-null  object\n",
            "dtypes: object(4)\n",
            "memory usage: 1.0+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test['sentiment'].value_counts(normalize=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "By9YYw5SyfEC",
        "outputId": "2291c107-368f-4380-c781-52f1c66e581d"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "neutral     0.404641\n",
              "positive    0.312111\n",
              "negative    0.283248\n",
              "Name: sentiment, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "neutral = train[train['sentiment']=='neutral']\n",
        "pos = train[train['sentiment']=='positive']\n",
        "neg = train[train['sentiment']=='negative']"
      ],
      "metadata": {
        "id": "StLPD9Fd2UOg"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clean training data**"
      ],
      "metadata": {
        "id": "QbW4UiEpq9kA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text to lowercase\n",
        "train['text'] = train['text'].str.lower()\n",
        "\n",
        "# Expand contractions e.g \"gonna\" to \"going to\" and \"i've\" to \"i have\"\n",
        "train['text'].replace( {r\"`\": \"'\"}, inplace= True, regex = True)\n",
        "train['text'] = train['text'].apply(contractions.fix)\n",
        "\n",
        "# Remove @, Unicode characters, punctuation, emojis, URLs, retweets, words with digits, and 1 or 2 letter words\n",
        "train['text'].replace( {r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?|\\w*\\d\\w*|\\b\\w{1,2}\\b\": \" \"}, inplace= True, regex = True)\n",
        "\n",
        "# Remove extra whitespaces\n",
        "train['text'].replace( {r\" +\": \" \"}, inplace= True, regex = True)\n",
        "train['text'] = train['text'].str.strip()\n",
        "\n",
        "# Correct spellings\n",
        "#spell = SpellChecker()\n",
        "\n",
        "#def correct_spellings(text):\n",
        "#    corrected_text = []\n",
        "#    misspelled_words = {}\n",
        "#    words = text.split()\n",
        "#    for w in spell.unknown(words):\n",
        "#        corr = spell.correction(w)\n",
        "#        if corr:\n",
        "#            misspelled_words[w] = spell.correction(w) or w\n",
        "#    corrected_text = [misspelled_words.get(w, w) for w in words]\n",
        "#    return \" \".join(corrected_text)\n",
        "\n",
        "#train['text'] = train['text'].apply(lambda x : correct_spellings(x))\n",
        "\n",
        "# Remove stopwords\n",
        "stop = stopwords.words('english')\n",
        "train['text'] = train['text'].apply(lambda text: \" \".join([word for word in text.split() if word not in (stop)]))\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "train['text'] = train['text'].apply(lambda text: \" \".join([stemmer.stem(word) for word in text.split()]))\n",
        "\n",
        "# Lemmatizing\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "train['text'] = train['text'].apply(lambda text: \" \".join([lemmatizer.lemmatize(word) for word in text.split()]))\n"
      ],
      "metadata": {
        "id": "5tDnIMd-q89D"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clean testing data**"
      ],
      "metadata": {
        "id": "-Npku_k2Tj5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text to lowercase\n",
        "test['text'] = test['text'].str.lower()\n",
        "\n",
        "# Expand contractions e.g \"gonna\" to \"going to\" and \"i've\" to \"i have\"\n",
        "test['text'].replace( {r\"`\": \"'\"}, inplace= True, regex = True)\n",
        "test['text'] = test['text'].apply(contractions.fix)\n",
        "\n",
        "# Remove @, Unicode characters, punctuation, emojis, URLs, retweets, words with digits, and 1 or 2 letter words\n",
        "test['text'].replace( {r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?|\\w*\\d\\w*|\\b\\w{1,2}\\b\": \" \"}, inplace= True, regex = True)\n",
        "\n",
        "# Remove extra whitespaces\n",
        "test['text'].replace( {r\" +\": \" \"}, inplace= True, regex = True)\n",
        "test['text'] = test['text'].str.strip()\n",
        "\n",
        "# Remove stopwords\n",
        "stop = stopwords.words('english')\n",
        "test['text'] = test['text'].apply(lambda text: \" \".join([word for word in text.split() if word not in (stop)]))\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "test['text'] = test['text'].apply(lambda text: \" \".join([stemmer.stem(word) for word in text.split()]))\n",
        "\n",
        "# Lemmatizing\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "test['text'] = test['text'].apply(lambda text: \" \".join([lemmatizer.lemmatize(word) for word in text.split()]))\n"
      ],
      "metadata": {
        "id": "dcgDVjv4TjSq"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train.head(20))\n",
        "print(train['text'].tail(20))"
      ],
      "metadata": {
        "id": "tT-89PPBe703",
        "outputId": "e1fd4259-a642-4d23-f39e-b819195ed32a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        textID                                               text  \\\n",
            "0   cb774db0d1                                   would respond go   \n",
            "1   549e992a42                            sooo sad miss san diego   \n",
            "2   088c60f138                                          bos bulli   \n",
            "3   9642c003ef                                interview leav alon   \n",
            "4   358bd9e861                son could put releas alreadi bought   \n",
            "5   28b57f3990             shameless plug best ranger forum earth   \n",
            "6   6e0c6d75b1                            feed babi fun smile coo   \n",
            "7   50e14c0bb8                                         soooo high   \n",
            "8   e050245fbd                                                      \n",
            "9   fc2cbefa9d              journey wow becam cooler hehe possibl   \n",
            "10  2339a9b08b  much love hope reckon chanc minim never go get...   \n",
            "11  16fab9f95b    realli realli like song love stori taylor swift   \n",
            "12  74a76f6e0a                          sharpi run danger low ink   \n",
            "13  04dd1d2e34                       want music tonight lost voic   \n",
            "14  bbe3cbf620                                          test test   \n",
            "15  8a939bfb59                                            sunburn   \n",
            "16  3440297f8b                         tri plot altern speak sigh   \n",
            "17  919fa93391   sick past day thu hair look wierd hat would look   \n",
            "18  af3fed7fc3                        back home go miss everi one   \n",
            "19  40e7becabf                                                 he   \n",
            "\n",
            "                                        selected_text sentiment  \n",
            "0                 I`d have responded, if I were going   neutral  \n",
            "1                                            Sooo SAD  negative  \n",
            "2                                         bullying me  negative  \n",
            "3                                      leave me alone  negative  \n",
            "4                                       Sons of ****,  negative  \n",
            "5   http://www.dothebouncy.com/smf - some shameles...   neutral  \n",
            "6                                                 fun  positive  \n",
            "7                                          Soooo high   neutral  \n",
            "8                                         Both of you   neutral  \n",
            "9                        Wow... u just became cooler.  positive  \n",
            "10  as much as i love to be hopeful, i reckon the ...   neutral  \n",
            "11                                               like  positive  \n",
            "12                                        DANGERously  negative  \n",
            "13                                               lost  negative  \n",
            "14                         test test from the LG enV2   neutral  \n",
            "15                              Uh oh, I am sunburned  negative  \n",
            "16                                             *sigh*  negative  \n",
            "17                                               sick  negative  \n",
            "18                                               onna  negative  \n",
            "19                         Hes just not that into you   neutral  \n",
            "27461       thank dear neighboor also gave coffe need best\n",
            "27462                          back bingo famili fun night\n",
            "27463      like drew said give chanc miss thoma move watch\n",
            "27464    rec game tri cri pain much need cannot lose he...\n",
            "27465                        sure tri keep enjoy studi see\n",
            "27466    naw pretti tame guy costum voyag style medic u...\n",
            "27467                   morn twit friend welcom new follow\n",
            "27468    grill mushroom oliv feta chees coffe breakfast...\n",
            "27469                                   day till come back\n",
            "27470        lol know haha fall asleep get bore shaun joke\n",
            "27471        defi graviti nobodi alll wizard ever go bring\n",
            "27472                                 want visit anim late\n",
            "27473           spoke yesterday respond girl wassup though\n",
            "27474    get earli feel good day walk work feel alright...\n",
            "27475                                          enjoy night\n",
            "27476    wish could come see denver husband lost job ca...\n",
            "27477    wonder rake client made clear net forc dev lea...\n",
            "27478    yay good enjoy break probabl need hectic weeke...\n",
            "27479                                                worth\n",
            "27480                           flirt go atg smile yay hug\n",
            "Name: text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Feature Selection**"
      ],
      "metadata": {
        "id": "SRySCY_E8Kke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorization parameters\n",
        "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
        "NGRAM_RANGE = (1, 2)  # Use 1-grams + 2-grams.\n",
        "\n",
        "# Limit on the number of features. We use the top 20K features.\n",
        "TOP_K = 20000\n",
        "\n",
        "# Whether text should be split into word or character n-grams.\n",
        "TOKEN_MODE = 'word' # Split text into word tokens.\n",
        "\n",
        "# Minimum document frequency below which a token will be discarded.\n",
        "MIN_DOCUMENT_FREQUENCY = 2\n",
        "\n",
        "def ngram_vectorize(train_texts, train_labels, test_texts):\n",
        "    \"\"\"Vectorizes texts as n-gram vectors.\n",
        "\n",
        "    1 text = 1 tf-idf vector the length of vocabulary of unigrams + bigrams.\n",
        "\n",
        "    # Arguments\n",
        "        train_texts: list, training text strings.\n",
        "        train_labels: np.ndarray, training labels.\n",
        "        test_texts: list, test text strings.\n",
        "\n",
        "    # Returns\n",
        "        train_vectors, test_vectors: vectorized training and test texts\n",
        "    \"\"\"\n",
        "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
        "    kwargs = {\n",
        "            'ngram_range': NGRAM_RANGE,\n",
        "            'analyzer': TOKEN_MODE,  \n",
        "            'min_df': MIN_DOCUMENT_FREQUENCY,\n",
        "            'sublinear_tf': 'True'\n",
        "    }\n",
        "    vectorizer = TfidfVectorizer(**kwargs)\n",
        "\n",
        "    # Learn vocabulary from training texts and vectorize training texts.\n",
        "    train_vectors = vectorizer.fit_transform(train_texts)\n",
        "\n",
        "    # Vectorize validation texts.\n",
        "    test_vectors = vectorizer.transform(test_texts)\n",
        "\n",
        "    # Select top 'k' of the vectorized features.\n",
        "    selector = SelectKBest(f_classif, k=min(TOP_K, train_vectors.shape[1]))\n",
        "    selector.fit(train_vectors, train_labels)\n",
        "    train_vectors = selector.transform(train_vectors).astype('float32').toarray()\n",
        "    test_vectors = selector.transform(test_vectors).astype('float32').toarray()\n",
        "    return train_vectors, test_vectors\n",
        "\n",
        "train_vectors, test_vectors = ngram_vectorize(train['text'], train['sentiment'], test['text'])\n",
        "\n",
        "\n",
        "# # Create feature vectors\n",
        "# vectorizer = TfidfVectorizer(min_df = 5,\n",
        "#                              max_df = 0.8,\n",
        "#                              sublinear_tf = True,\n",
        "#                              use_idf = True)\n",
        "# train_vectors = vectorizer.fit_transform(train['text'])\n",
        "# test_vectors = vectorizer.transform(test['text'])"
      ],
      "metadata": {
        "id": "I3KHg2k7kGyh"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Supervised modelling**"
      ],
      "metadata": {
        "id": "qSC6vhwKaAb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# supervised_models = [\n",
        "#     LinearSVC(),\n",
        "#     #SVC(kernel='linear'),\n",
        "#     #XGBClassifier(objective='multi:softmax'),\n",
        "# ]\n",
        "# # 5-fold Cross-validation\n",
        "# k = 5\n",
        "# cv_df = pd.DataFrame(index=range(k * len(supervised_models)))\n",
        "# entries = []\n",
        "# scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
        "# for model in supervised_models:\n",
        "#   model_name = model.__class__.__name__\n",
        "#   scores = cross_validate(model, train_vectors, train['sentiment'], scoring=scoring, cv=k)\n",
        "#   tmp_scores = zip([model_name]*k, range(k), scores['test_accuracy'], scores['test_f1_macro'])\n",
        "#   entries.extend(list(tmp_scores))\n",
        "# cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_id', 'accuracy', 'f1_score'])"
      ],
      "metadata": {
        "id": "WKzE_70_F3dT"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mean_accuracy = cv_df.groupby('model_name').accuracy.mean()\n",
        "# std_accuracy = cv_df.groupby('model_name').accuracy.std()\n",
        "\n",
        "# mean_f1 = cv_df.groupby('model_name').f1_score.mean()\n",
        "# std_f1 = cv_df.groupby('model_name').f1_score.std()\n",
        "\n",
        "# acc = pd.concat([mean_accuracy, std_accuracy, mean_f1, std_f1], axis= 1, \n",
        "#           ignore_index=True)\n",
        "# acc.columns = ['Accuracy', ' Std dev', ' F1 score', ' Std dev']\n",
        "# acc"
      ],
      "metadata": {
        "id": "TE3JzHbFR2lC"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = LinearSVC(loss='hinge', max_iter=10000)\n",
        "# model.fit(train_vectors, train['sentiment'])\n",
        "# prediction = model.predict(test_vectors)\n",
        "# print(f\"Test set accuracy: {accuracy_score(test['sentiment'], prediction) * 100} %\\n\")\n"
      ],
      "metadata": {
        "id": "9iWtXl49SIad"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Classification report\n",
        "# print('\\tClassification Metrics - LinearSVC\\n')\n",
        "# print(classification_report(test['sentiment'], prediction, target_names= ['negative', 'neutral', 'positive']))"
      ],
      "metadata": {
        "id": "jfBEYCs06XWc"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data = confusion_matrix(test['sentiment'], prediction)\n",
        "# disp = ConfusionMatrixDisplay(confusion_matrix=data, display_labels=model.classes_)\n",
        "# disp.plot(cmap=\"Blues\")\n",
        "# plt.ylabel('ACTUAL')\n",
        "# plt.xlabel('\\nPREDICTED')\n",
        "# plt.title(\"\\nCONFUSION MATRIX - LinearSVC\\n\");\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "35Inx_EO6nKD"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = XGBClassifier(objective='multi:softmax')\n",
        "# model.fit(train_vectors, train['sentiment'])\n",
        "# prediction = model.predict(test_vectors)\n",
        "# print(f\"Test set accuracy: {accuracy_score(test['sentiment'], prediction) * 100} %\\n\")"
      ],
      "metadata": {
        "id": "mmrYfV82SwfJ"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Classification report\n",
        "# print('\\tCLASSIFICATIION METRICS - XGBClassifier\\n')\n",
        "# print(classification_report(test['sentiment'], prediction, target_names= ['negative', 'neutral', 'positive']))"
      ],
      "metadata": {
        "id": "4J1rj4D6TAQq"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data = confusion_matrix(test['sentiment'], prediction)\n",
        "# disp = ConfusionMatrixDisplay(confusion_matrix=data, display_labels=model.classes_)\n",
        "# disp.plot(cmap=\"Blues\")\n",
        "# plt.ylabel('ACTUAL')\n",
        "# plt.xlabel('\\nPREDICTED')\n",
        "# plt.title(\"\\nCONFUSION MATRIX - XGBClassifier\\n\");\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "B-fd1cE6WRcx"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unsupervised modelling**"
      ],
      "metadata": {
        "id": "fp5HLVdNboZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp_model(layers, units, op_units, op_activation, dropout_rate, input_shape, num_classes):\n",
        "    \"\"\"Creates an instance of a multi-layer perceptron model.\n",
        "\n",
        "    # Arguments\n",
        "        layers: int, number of `Dense` layers in the model.\n",
        "        units: int, output dimension of the layers.\n",
        "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
        "        input_shape: tuple, shape of input to the model.\n",
        "        num_classes: int, number of output classes.\n",
        "\n",
        "    # Returns\n",
        "        An MLP model instance.\n",
        "    \"\"\"\n",
        "\n",
        "    model = models.Sequential()\n",
        "    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n",
        "\n",
        "    for _ in range(layers-1):\n",
        "        model.add(Dense(units=units, activation='relu'))\n",
        "        model.add(Dropout(rate=dropout_rate))\n",
        "\n",
        "    model.add(Dense(units=op_units, activation=op_activation))\n",
        "    return model\n",
        "\n",
        "\n",
        "# # 2 layer MLP\n",
        "# layers = 2\n",
        "# num_classes = 3\n",
        "# dropout_rate = 0.2\n",
        "# input_shape = train_vectors.shape[1:]\n",
        "\n",
        "# # Units and activation function for the last network layer.\n",
        "# activation = 'softmax'\n",
        "# units = num_classes\n",
        "\n",
        "# model = mlp_model(layers, units, activation, dropout_rate, input_shape, num_classes)"
      ],
      "metadata": {
        "id": "lyYc_8N3bkb7"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Compile model with learning parameters.\n",
        "\n",
        "# batch_size=128\n",
        "# epochs=1000\n",
        "# learning_rate=1e-3\n",
        "\n",
        "# loss = 'sparse_categorical_crossentropy' # for multiclass\n",
        "# optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
        "# model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
        "\n",
        "# # Create callback for early stopping on validation loss. If the loss does\n",
        "# # not decrease in two consecutive tries, stop training.\n",
        "# callbacks = [tf.keras.callbacks.EarlyStopping(\n",
        "#     monitor='val_loss', patience=2)]\n",
        "\n",
        "# # Train and test model.\n",
        "# history = model.fit(\n",
        "#         train_vectors,\n",
        "#         train[\"sentiment\"],\n",
        "#         epochs=epochs,\n",
        "#         callbacks=callbacks,\n",
        "#         validation_data=(test_vectors, train[\"sentiment\"]),\n",
        "#         verbose=2,  # Logs once per epoch.\n",
        "#         batch_size=batch_size)\n",
        "\n",
        "# # Print results.\n",
        "# history = history.history\n",
        "# print('Test accuracy: {acc}, loss: {loss}'.format(\n",
        "#         acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
        "\n",
        "# # Save model.\n",
        "# model.save('Twitter_mlp_model.h5')"
      ],
      "metadata": {
        "id": "_d1km1qHkNfS"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train model**"
      ],
      "metadata": {
        "id": "0ujbGFwieJlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ngram_model(train_vectors, train_labels, test_vectors, test_labels,\n",
        "                      num_classes,\n",
        "                      learning_rate=1e-3,\n",
        "                      epochs=1000,\n",
        "                      batch_size=128,\n",
        "                      layers=2,\n",
        "                      units=64,\n",
        "                      dropout_rate=0.2):\n",
        "    \"\"\"Trains n-gram model on the given dataset.\n",
        "\n",
        "    # Arguments\n",
        "        data: tuples of training and test texts and labels.\n",
        "        learning_rate: float, learning rate for training model.\n",
        "        epochs: int, number of epochs.\n",
        "        batch_size: int, number of samples per batch.\n",
        "        layers: int, number of `Dense` layers in the model.\n",
        "        units: int, output dimension of Dense layers in the model.\n",
        "        dropout_rate: float: percentage of input to drop at Dropout layers.\n",
        "\n",
        "    # Raises\n",
        "        ValueError: If validation data has label values which were not seen\n",
        "            in the training data.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create model instance.\n",
        "    model = mlp_model(layers=layers, units=units, \n",
        "                      op_units=num_classes, op_activation = 'softmax',\n",
        "                      dropout_rate=dropout_rate,\n",
        "                      input_shape=train_vectors.shape[1:],\n",
        "                      num_classes=num_classes)\n",
        "\n",
        "    # Compile model with learning parameters.\n",
        "    loss = 'sparse_categorical_crossentropy' # for multiclass\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
        "\n",
        "    # Create callback for early stopping on validation loss. If the loss does\n",
        "    # not decrease in two consecutive tries, stop training.\n",
        "    callbacks = [EarlyStopping(monitor='val_loss', patience=2)]\n",
        "\n",
        "    # Train and test model.\n",
        "    history = model.fit(\n",
        "            train_vectors,\n",
        "            train_labels,\n",
        "            epochs=epochs,\n",
        "            callbacks=callbacks,\n",
        "            validation_data=(test_vectors, test_labels),\n",
        "            verbose=2,  # Logs once per epoch.\n",
        "            batch_size=batch_size)\n",
        "\n",
        "    # Print results.\n",
        "    history = history.history\n",
        "    print('Test accuracy: {acc}, loss: {loss}'.format(\n",
        "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
        "\n",
        "    # Save model.\n",
        "    model.save('Twitter_mlp_model.h5')\n",
        "    return history['val_acc'][-1], history['val_loss'][-1]"
      ],
      "metadata": {
        "id": "F5YNy00ieJCG"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "labels = np.array(train['sentiment'])\n",
        "train_labels = []\n",
        "for i in range(len(labels)):\n",
        "    if labels[i] == 'neutral':\n",
        "        train_labels.append(0)\n",
        "    if labels[i] == 'negative':\n",
        "        train_labels.append(1)\n",
        "    if labels[i] == 'positive':\n",
        "        train_labels.append(2)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "labels = np.array(test['sentiment'])\n",
        "test_labels = []\n",
        "for i in range(len(labels)):\n",
        "    if labels[i] == 'neutral':\n",
        "        test_labels.append(0)\n",
        "    if labels[i] == 'negative':\n",
        "        test_labels.append(1)\n",
        "    if labels[i] == 'positive':\n",
        "        test_labels.append(2)\n",
        "test_labels = np.array(test_labels)"
      ],
      "metadata": {
        "id": "5i9e7-1Ys-7W"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_units=64\n",
        "\n",
        "train_ngram_model(train_vectors, train_labels, test_vectors, test_labels,\n",
        "                  num_classes=3,\n",
        "                  learning_rate=1e-3,\n",
        "                  epochs=1000,\n",
        "                  batch_size=128,\n",
        "                  layers=2,\n",
        "                  units=hidden_units,\n",
        "                  dropout_rate=0.2)"
      ],
      "metadata": {
        "id": "EjbkRki7hP07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b652210e-7222-4602-d3c6-beb9366f3c24"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "215/215 - 13s - loss: 0.9660 - acc: 0.5431 - val_loss: 0.9001 - val_acc: 0.5925\n",
            "Epoch 2/1000\n",
            "215/215 - 10s - loss: 0.7087 - acc: 0.7151 - val_loss: 0.8591 - val_acc: 0.6081\n",
            "Epoch 3/1000\n",
            "215/215 - 10s - loss: 0.5858 - acc: 0.7694 - val_loss: 0.8814 - val_acc: 0.6024\n",
            "Epoch 4/1000\n",
            "215/215 - 11s - loss: 0.5093 - acc: 0.8033 - val_loss: 0.9105 - val_acc: 0.5948\n",
            "Test accuracy: 0.594793438911438, loss: 0.9105126261711121\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.594793438911438, 0.9105126261711121)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    }
  ]
}