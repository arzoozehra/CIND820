{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arzoozehra/CIND820/blob/main/xgboost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import libraries**"
      ],
      "metadata": {
        "id": "-bkY_pYm8WwM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Wr2dHIs7WLWJ",
        "outputId": "fc7fecd9-cbbc-4500-e0fc-23db88e08a4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import time\n",
        "#!pip install contractions\n",
        "import contractions\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "#!pip install pyspellchecker\n",
        "#from spellchecker import SpellChecker\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from collections import Counter\n",
        "import plotly.express as px\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from xgboost import XGBClassifier\n",
        "from tensorflow.python.keras import models\n",
        "from tensorflow.python.keras.layers import Dense, Dropout\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping\n",
        "from tensorflow.python.keras.optimizer_v2.adam import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load data**"
      ],
      "metadata": {
        "id": "GfYAFFPs8gNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://raw.githubusercontent.com/arzoozehra/CIND820/main/data/train.csv'\n",
        "train = pd.read_csv(url)\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/arzoozehra/CIND820/main/data/test.csv')\n",
        "\n",
        "# Remove row with missing values\n",
        "train.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "WzPmmeuGrev1"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train[\"text\"].head(10))\n",
        "print(train[\"text\"].tail(10))"
      ],
      "metadata": {
        "id": "rQwxYsauWPT1",
        "outputId": "1351042a-d119-4430-a057-b8ab7300b614",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                  I`d have responded, if I were going\n",
            "1        Sooo SAD I will miss you here in San Diego!!!\n",
            "2                            my boss is bullying me...\n",
            "3                       what interview! leave me alone\n",
            "4     Sons of ****, why couldn`t they put them on t...\n",
            "5    http://www.dothebouncy.com/smf - some shameles...\n",
            "6    2am feedings for the baby are fun when he is a...\n",
            "7                                           Soooo high\n",
            "8                                          Both of you\n",
            "9     Journey!? Wow... u just became cooler.  hehe....\n",
            "Name: text, dtype: object\n",
            "27471    i`m defying gravity. and nobody in alll of oz,...\n",
            "27472    http://twitpic.com/663vr - Wanted to visit the...\n",
            "27473     in spoke to you yesterday and u didnt respond...\n",
            "27474    So I get up early and I feel good about the da...\n",
            "27475                                       enjoy ur night\n",
            "27476     wish we could come see u on Denver  husband l...\n",
            "27477     I`ve wondered about rake to.  The client has ...\n",
            "27478     Yay good for both of you. Enjoy the break - y...\n",
            "27479                           But it was worth it  ****.\n",
            "27480       All this flirting going on - The ATG smiles...\n",
            "Name: text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clean training data**"
      ],
      "metadata": {
        "id": "QbW4UiEpq9kA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text to lowercase\n",
        "train['text'] = train['text'].str.lower()\n",
        "\n",
        "# Expand contractions e.g \"gonna\" to \"going to\" and \"i've\" to \"i have\"\n",
        "train['text'].replace( {r\"`\": \"'\"}, inplace= True, regex = True)\n",
        "train['text'] = train['text'].apply(contractions.fix)\n",
        "\n",
        "# Remove @, Unicode characters, punctuation, emojis, URLs, retweets, words with digits, and 1 or 2 letter words\n",
        "train['text'].replace( {r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?|\\w*\\d\\w*|\\b\\w{1,2}\\b\": \" \"}, inplace= True, regex = True)\n",
        "\n",
        "# Remove extra whitespaces\n",
        "train['text'].replace( {r\" +\": \" \"}, inplace= True, regex = True)\n",
        "train['text'] = train['text'].str.strip()\n",
        "\n",
        "# Correct spellings\n",
        "#spell = SpellChecker()\n",
        "\n",
        "#def correct_spellings(text):\n",
        "#    corrected_text = []\n",
        "#    misspelled_words = {}\n",
        "#    words = text.split()\n",
        "#    for w in spell.unknown(words):\n",
        "#        corr = spell.correction(w)\n",
        "#        if corr:\n",
        "#            misspelled_words[w] = spell.correction(w) or w\n",
        "#    corrected_text = [misspelled_words.get(w, w) for w in words]\n",
        "#    return \" \".join(corrected_text)\n",
        "\n",
        "#train['text'] = train['text'].apply(lambda x : correct_spellings(x))\n",
        "\n",
        "# Remove stopwords\n",
        "stop = stopwords.words('english')\n",
        "train['text'] = train['text'].apply(lambda text: \" \".join([word for word in text.split() if word not in (stop)]))\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "train['text'] = train['text'].apply(lambda text: \" \".join([stemmer.stem(word) for word in text.split()]))\n",
        "\n",
        "# Lemmatizing\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "train['text'] = train['text'].apply(lambda text: \" \".join([lemmatizer.lemmatize(word) for word in text.split()]))\n"
      ],
      "metadata": {
        "id": "5tDnIMd-q89D"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clean testing data**"
      ],
      "metadata": {
        "id": "-Npku_k2Tj5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text to lowercase\n",
        "test['text'] = test['text'].str.lower()\n",
        "\n",
        "# Expand contractions e.g \"gonna\" to \"going to\" and \"i've\" to \"i have\"\n",
        "test['text'].replace( {r\"`\": \"'\"}, inplace= True, regex = True)\n",
        "test['text'] = test['text'].apply(contractions.fix)\n",
        "\n",
        "# Remove @, Unicode characters, punctuation, emojis, URLs, retweets, words with digits, and 1 or 2 letter words\n",
        "test['text'].replace( {r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?|\\w*\\d\\w*|\\b\\w{1,2}\\b\": \" \"}, inplace= True, regex = True)\n",
        "\n",
        "# Remove extra whitespaces\n",
        "test['text'].replace( {r\" +\": \" \"}, inplace= True, regex = True)\n",
        "test['text'] = test['text'].str.strip()\n",
        "\n",
        "# Remove stopwords\n",
        "stop = stopwords.words('english')\n",
        "test['text'] = test['text'].apply(lambda text: \" \".join([word for word in text.split() if word not in (stop)]))\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "test['text'] = test['text'].apply(lambda text: \" \".join([stemmer.stem(word) for word in text.split()]))\n",
        "\n",
        "# Lemmatizing\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "test['text'] = test['text'].apply(lambda text: \" \".join([lemmatizer.lemmatize(word) for word in text.split()]))\n"
      ],
      "metadata": {
        "id": "dcgDVjv4TjSq"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train[\"text\"].head(10))\n",
        "print(train[\"text\"].tail(10))"
      ],
      "metadata": {
        "id": "tT-89PPBe703",
        "outputId": "e2d6a5fc-b05b-4c64-cb8a-819a58278f49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                          would respond go\n",
            "1                   sooo sad miss san diego\n",
            "2                                 bos bulli\n",
            "3                       interview leav alon\n",
            "4       son could put releas alreadi bought\n",
            "5    shameless plug best ranger forum earth\n",
            "6                   feed babi fun smile coo\n",
            "7                                soooo high\n",
            "8                                          \n",
            "9     journey wow becam cooler hehe possibl\n",
            "Name: text, dtype: object\n",
            "27471        defi graviti nobodi alll wizard ever go bring\n",
            "27472                                 want visit anim late\n",
            "27473           spoke yesterday respond girl wassup though\n",
            "27474    get earli feel good day walk work feel alright...\n",
            "27475                                          enjoy night\n",
            "27476    wish could come see denver husband lost job ca...\n",
            "27477    wonder rake client made clear net forc dev lea...\n",
            "27478    yay good enjoy break probabl need hectic weeke...\n",
            "27479                                                worth\n",
            "27480                           flirt go atg smile yay hug\n",
            "Name: text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test['text'].head(20))\n",
        "print(test['text'].tail(20))"
      ],
      "metadata": {
        "id": "1C8JW4YmUcbO",
        "outputId": "cae30879-16f6-43ea-ad3c-2ef1ebbaa7ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                                      last session day\n",
            "1     shanghai also realli excit precis skyscrap gal...\n",
            "2     recess hit veroniqu branquinho quit compani shame\n",
            "3                                        happi birthday\n",
            "4                                                  like\n",
            "5                                    great weee visitor\n",
            "6                                think everyon hate lol\n",
            "7        soooooo wish could school myspac complet block\n",
            "8                           within short time last clue\n",
            "9     get day alright done anyth yet leav soon steps...\n",
            "10                bike put hold known argh total bummer\n",
            "11                                            check win\n",
            "12                             twitter tavern bore much\n",
            "13    weekend youngest son turn tomorrow make kind s...\n",
            "14          come socket feel like phone hole virgin loo\n",
            "15               hot today like hate new timet bad week\n",
            "16                                                 miss\n",
            "17                                                cramp\n",
            "18          guy say answer question yesterday nice song\n",
            "19          go spiritu stagnent explod ego realis great\n",
            "Name: text, dtype: object\n",
            "3514                                        notic ridicul\n",
            "3515                        ride high low mood chore blow\n",
            "3516            hate websit say ticket price anoth websit\n",
            "3517                                          brake wrrkk\n",
            "3518                     outta follow talk lol shi biteee\n",
            "3519    ye thank haha field flower exist singapor well...\n",
            "3520                                song site want comput\n",
            "3521                      munchin bacon butti woohoo fave\n",
            "3522           school today teacher cancel lesson chillin\n",
            "3523    eye start hurt late must reach updat due tweet...\n",
            "3524    hair dresser pas away yesterday breast cancer ...\n",
            "3525    suppos tomorrow tooooo use ga ticket money pay...\n",
            "3526               best thing ever done carri birth child\n",
            "3527    mother citi terrel texa district citi council ...\n",
            "3528                                     friday even idea\n",
            "3529                                tire cannot sleep tri\n",
            "3530    alon old hous thank net keep aliv kick whoever...\n",
            "3531    know mean littl dog sink depress want move som...\n",
            "3532                sutra next youtub video go love video\n",
            "3533                                  omgssh ang cute bbi\n",
            "Name: text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Feature Selection**"
      ],
      "metadata": {
        "id": "SRySCY_E8Kke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorization parameters\n",
        "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
        "NGRAM_RANGE = (1, 2)  # Use 1-grams + 2-grams.\n",
        "\n",
        "# Limit on the number of features. We use the top 20K features.\n",
        "TOP_K = 20000\n",
        "\n",
        "# Whether text should be split into word or character n-grams.\n",
        "TOKEN_MODE = 'word' # Split text into word tokens.\n",
        "\n",
        "# Minimum document frequency below which a token will be discarded.\n",
        "MIN_DOCUMENT_FREQUENCY = 2\n",
        "\n",
        "def ngram_vectorize(train_texts, train_labels, test_texts):\n",
        "    \"\"\"Vectorizes texts as n-gram vectors.\n",
        "\n",
        "    1 text = 1 tf-idf vector the length of vocabulary of unigrams + bigrams.\n",
        "\n",
        "    # Arguments\n",
        "        train_texts: list, training text strings.\n",
        "        train_labels: np.ndarray, training labels.\n",
        "        test_texts: list, test text strings.\n",
        "\n",
        "    # Returns\n",
        "        train_vectors, test_vectors: vectorized training and test texts\n",
        "    \"\"\"\n",
        "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
        "    kwargs = {\n",
        "            'ngram_range': NGRAM_RANGE,\n",
        "            'analyzer': TOKEN_MODE,  \n",
        "            'min_df': MIN_DOCUMENT_FREQUENCY,\n",
        "            'sublinear_tf': 'True'\n",
        "    }\n",
        "    vectorizer = TfidfVectorizer(**kwargs)\n",
        "\n",
        "    # Learn vocabulary from training texts and vectorize training texts.\n",
        "    train_vectors = vectorizer.fit_transform(train_texts)\n",
        "\n",
        "    # Vectorize validation texts.\n",
        "    test_vectors = vectorizer.transform(test_texts)\n",
        "\n",
        "    # Select top 'k' of the vectorized features.\n",
        "    selector = SelectKBest(f_classif, k=min(TOP_K, train_vectors.shape[1]))\n",
        "    selector.fit(train_vectors, train_labels)\n",
        "    train_vectors = selector.transform(train_vectors).astype('float32').toarray()\n",
        "    test_vectors = selector.transform(test_vectors).astype('float32').toarray()\n",
        "    return train_vectors, test_vectors\n",
        "\n",
        "train_vectors, test_vectors = ngram_vectorize(train['text'], train['sentiment'], test['text'])\n",
        "\n",
        "\n",
        "# # Create feature vectors\n",
        "# vectorizer = TfidfVectorizer(min_df = 5,\n",
        "#                              max_df = 0.8,\n",
        "#                              sublinear_tf = True,\n",
        "#                              use_idf = True)\n",
        "# train_vectors = vectorizer.fit_transform(train['text'])\n",
        "# test_vectors = vectorizer.transform(test['text'])"
      ],
      "metadata": {
        "id": "I3KHg2k7kGyh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Supervised modelling**"
      ],
      "metadata": {
        "id": "qSC6vhwKaAb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "supervised_models = [\n",
        "    LinearSVC(),\n",
        "    SVC(kernel='linear'),\n",
        "    XGBClassifier(objective='multi:softmax'),\n",
        "]\n",
        "# 5-fold Cross-validation\n",
        "k = 5\n",
        "cv_df = pd.DataFrame(index=range(k * len(supervised_models)))\n",
        "entries = []\n",
        "scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
        "for model in supervised_models:\n",
        "  model_name = model.__class__.__name__\n",
        "  scores = cross_validate(model, train_vectors, train['sentiment'], scoring=scoring, cv=k)\n",
        "  tmp_scores = zip([model_name]*k, range(k), scores['test_accuracy'], scores['test_f1_macro'])\n",
        "  entries.extend(list(tmp_scores))\n",
        "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_id', 'accuracy', 'f1_score'])"
      ],
      "metadata": {
        "id": "WKzE_70_F3dT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_accuracy = cv_df.groupby('model_name').accuracy.mean()\n",
        "std_accuracy = cv_df.groupby('model_name').accuracy.std()\n",
        "\n",
        "mean_f1 = cv_df.groupby('model_name').f1_score.mean()\n",
        "std_f1 = cv_df.groupby('model_name').f1_score.std()\n",
        "\n",
        "acc = pd.concat([mean_accuracy, std_accuracy, mean_f1, std_f1], axis= 1, \n",
        "          ignore_index=True)\n",
        "acc.columns = ['Accuracy', ' Std dev', ' F1 score', ' Std dev']\n",
        "acc"
      ],
      "metadata": {
        "id": "TE3JzHbFR2lC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearSVC(loss='hinge', max_iter=10000)\n",
        "model.fit(train_vectors, train['sentiment'])\n",
        "prediction = model.predict(test_vectors)\n",
        "print(f\"Test set accuracy: {accuracy_score(test['sentiment'], prediction) * 100} %\\n\")"
      ],
      "metadata": {
        "id": "9iWtXl49SIad"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report\n",
        "print('\\tClassification Metrics - LinearSVC\\n')\n",
        "print(classification_report(test['sentiment'], prediction, target_names= ['negative', 'neutral', 'positive']))"
      ],
      "metadata": {
        "id": "jfBEYCs06XWc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = confusion_matrix(test['sentiment'], prediction)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=data, display_labels=model.classes_)\n",
        "disp.plot(cmap=\"Blues\")\n",
        "plt.ylabel('ACTUAL')\n",
        "plt.xlabel('\\nPREDICTED')\n",
        "plt.title(\"\\nCONFUSION MATRIX - LinearSVC\\n\");\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "35Inx_EO6nKD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = XGBClassifier(objective='multi:softmax')\n",
        "model.fit(train_vectors, train['sentiment'])\n",
        "prediction = model.predict(test_vectors)\n",
        "print(f\"Test set accuracy: {accuracy_score(test['sentiment'], prediction) * 100} %\\n\")"
      ],
      "metadata": {
        "id": "mmrYfV82SwfJ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report\n",
        "print('\\tCLASSIFICATIION METRICS - XGBClassifier\\n')\n",
        "print(classification_report(test['sentiment'], prediction, target_names= ['negative', 'neutral', 'positive']))"
      ],
      "metadata": {
        "id": "4J1rj4D6TAQq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = confusion_matrix(test['sentiment'], prediction)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=data, display_labels=model.classes_)\n",
        "disp.plot(cmap=\"Blues\")\n",
        "plt.ylabel('ACTUAL')\n",
        "plt.xlabel('\\nPREDICTED')\n",
        "plt.title(\"\\nCONFUSION MATRIX - XGBClassifier\\n\");\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B-fd1cE6WRcx"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unsupervised modelling**"
      ],
      "metadata": {
        "id": "fp5HLVdNboZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp_model(layers, units, op_units, op_activation, dropout_rate, input_shape, num_classes):\n",
        "    \"\"\"Creates an instance of a multi-layer perceptron model.\n",
        "\n",
        "    # Arguments\n",
        "        layers: int, number of `Dense` layers in the model.\n",
        "        units: int, output dimension of the layers.\n",
        "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
        "        input_shape: tuple, shape of input to the model.\n",
        "        num_classes: int, number of output classes.\n",
        "\n",
        "    # Returns\n",
        "        An MLP model instance.\n",
        "    \"\"\"\n",
        "\n",
        "    model = models.Sequential()\n",
        "    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n",
        "\n",
        "    for _ in range(layers-1):\n",
        "        model.add(Dense(units=units, activation='relu'))\n",
        "        model.add(Dropout(rate=dropout_rate))\n",
        "\n",
        "    model.add(Dense(units=op_units, activation=op_activation))\n",
        "    return model"
      ],
      "metadata": {
        "id": "lyYc_8N3bkb7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train model**"
      ],
      "metadata": {
        "id": "0ujbGFwieJlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ngram_model(train_vectors, train_labels, test_vectors, test_labels,\n",
        "                      num_classes,\n",
        "                      learning_rate=1e-3,\n",
        "                      epochs=1000,\n",
        "                      batch_size=128,\n",
        "                      layers=2,\n",
        "                      units=64,\n",
        "                      dropout_rate=0.2):\n",
        "    \"\"\"Trains n-gram model on the given dataset.\n",
        "\n",
        "    # Arguments\n",
        "        data: tuples of training and test texts and labels.\n",
        "        learning_rate: float, learning rate for training model.\n",
        "        epochs: int, number of epochs.\n",
        "        batch_size: int, number of samples per batch.\n",
        "        layers: int, number of `Dense` layers in the model.\n",
        "        units: int, output dimension of Dense layers in the model.\n",
        "        dropout_rate: float: percentage of input to drop at Dropout layers.\n",
        "\n",
        "    # Raises\n",
        "        ValueError: If validation data has label values which were not seen\n",
        "            in the training data.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create model instance.\n",
        "    model = mlp_model(layers=layers, units=units, \n",
        "                      op_units=num_classes, op_activation = 'softmax',\n",
        "                      dropout_rate=dropout_rate,\n",
        "                      input_shape=train_vectors.shape[1:],\n",
        "                      num_classes=num_classes)\n",
        "\n",
        "    # Compile model with learning parameters.\n",
        "    loss = 'sparse_categorical_crossentropy' # for multiclass\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
        "\n",
        "    # Create callback for early stopping on validation loss. If the loss does\n",
        "    # not decrease in two consecutive tries, stop training.\n",
        "    callbacks = [EarlyStopping(monitor='val_loss', patience=2)]\n",
        "\n",
        "    # Train and test model.\n",
        "    history = model.fit(\n",
        "            train_vectors,\n",
        "            train_labels,\n",
        "            epochs=epochs,\n",
        "            callbacks=callbacks,\n",
        "            validation_data=(test_vectors, test_labels),\n",
        "            verbose=2,  # Logs once per epoch.\n",
        "            batch_size=batch_size)\n",
        "\n",
        "    # Print results.\n",
        "    history = history.history\n",
        "    print('Test accuracy: {acc}, loss: {loss}'.format(\n",
        "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
        "\n",
        "    # Save model.\n",
        "    model.save('Twitter_mlp_model.h5')\n",
        "    return history['val_acc'][-1], history['val_loss'][-1]"
      ],
      "metadata": {
        "id": "F5YNy00ieJCG"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = np.array(train['sentiment'])\n",
        "train_labels = []\n",
        "for i in range(len(labels)):\n",
        "    if labels[i] == 'neutral':\n",
        "        train_labels.append(0)\n",
        "    if labels[i] == 'negative':\n",
        "        train_labels.append(1)\n",
        "    if labels[i] == 'positive':\n",
        "        train_labels.append(2)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "labels = np.array(test['sentiment'])\n",
        "test_labels = []\n",
        "for i in range(len(labels)):\n",
        "    if labels[i] == 'neutral':\n",
        "        test_labels.append(0)\n",
        "    if labels[i] == 'negative':\n",
        "        test_labels.append(1)\n",
        "    if labels[i] == 'positive':\n",
        "        test_labels.append(2)\n",
        "test_labels = np.array(test_labels)"
      ],
      "metadata": {
        "id": "5i9e7-1Ys-7W"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_units=64\n",
        "\n",
        "train_ngram_model(train_vectors, train_labels, test_vectors, test_labels,\n",
        "                  num_classes=3,\n",
        "                  learning_rate=1e-3,\n",
        "                  epochs=1000,\n",
        "                  batch_size=128,\n",
        "                  layers=2,\n",
        "                  units=hidden_units,\n",
        "                  dropout_rate=0.2)"
      ],
      "metadata": {
        "id": "EjbkRki7hP07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9ece35e-c72e-4340-c5bf-7dedf2a4dd49"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "215/215 - 11s - loss: 0.9651 - acc: 0.5451 - val_loss: 0.7958 - val_acc: 0.6896\n",
            "Epoch 2/1000\n",
            "215/215 - 10s - loss: 0.7109 - acc: 0.7127 - val_loss: 0.7093 - val_acc: 0.7057\n",
            "Epoch 3/1000\n",
            "215/215 - 10s - loss: 0.5901 - acc: 0.7680 - val_loss: 0.7088 - val_acc: 0.6938\n",
            "Epoch 4/1000\n",
            "215/215 - 10s - loss: 0.5114 - acc: 0.8062 - val_loss: 0.7242 - val_acc: 0.6851\n",
            "Epoch 5/1000\n",
            "215/215 - 10s - loss: 0.4548 - acc: 0.8277 - val_loss: 0.7557 - val_acc: 0.6737\n",
            "Test accuracy: 0.673740804195404, loss: 0.7556890249252319\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.673740804195404, 0.7556890249252319)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}
