{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arzoozehra/CIND820/blob/main/models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import libraries**"
      ],
      "metadata": {
        "id": "-bkY_pYm8WwM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wr2dHIs7WLWJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import time\n",
        "#!pip install contractions\n",
        "import contractions\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "#!pip install pyspellchecker\n",
        "#from spellchecker import SpellChecker\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from collections import Counter\n",
        "import plotly.express as px\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from xgboost import XGBClassifier\n",
        "from tensorflow.python.keras import models\n",
        "from tensorflow.python.keras.layers import Dense, Dropout\n",
        "from tensorflow.python.keras.callbacks import EarlyStopping\n",
        "from tensorflow.python.keras.optimizer_v2.adam import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load data**"
      ],
      "metadata": {
        "id": "GfYAFFPs8gNl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://raw.githubusercontent.com/arzoozehra/CIND820/main/data/train.csv'\n",
        "train = pd.read_csv(url)\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/arzoozehra/CIND820/main/data/test.csv')\n",
        "\n",
        "# Remove row with missing values\n",
        "train.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "WzPmmeuGrev1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train[\"text\"].head(10))\n",
        "print(train[\"text\"].tail(10))"
      ],
      "metadata": {
        "id": "rQwxYsauWPT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clean training data**"
      ],
      "metadata": {
        "id": "QbW4UiEpq9kA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text to lowercase\n",
        "train['text'] = train['text'].str.lower()\n",
        "\n",
        "# Expand contractions e.g \"gonna\" to \"going to\" and \"i've\" to \"i have\"\n",
        "train['text'].replace( {r\"`\": \"'\"}, inplace= True, regex = True)\n",
        "train['text'] = train['text'].apply(contractions.fix)\n",
        "\n",
        "# Remove @, Unicode characters, punctuation, emojis, URLs, retweets, words with digits, and 1 or 2 letter words\n",
        "train['text'].replace( {r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?|\\w*\\d\\w*|\\b\\w{1,2}\\b\": \" \"}, inplace= True, regex = True)\n",
        "\n",
        "# Remove extra whitespaces\n",
        "train['text'].replace( {r\" +\": \" \"}, inplace= True, regex = True)\n",
        "train['text'] = train['text'].str.strip()\n",
        "\n",
        "# Correct spellings\n",
        "#spell = SpellChecker()\n",
        "\n",
        "#def correct_spellings(text):\n",
        "#    corrected_text = []\n",
        "#    misspelled_words = {}\n",
        "#    words = text.split()\n",
        "#    for w in spell.unknown(words):\n",
        "#        corr = spell.correction(w)\n",
        "#        if corr:\n",
        "#            misspelled_words[w] = spell.correction(w) or w\n",
        "#    corrected_text = [misspelled_words.get(w, w) for w in words]\n",
        "#    return \" \".join(corrected_text)\n",
        "\n",
        "#train['text'] = train['text'].apply(lambda x : correct_spellings(x))\n",
        "\n",
        "# Remove stopwords\n",
        "stop = stopwords.words('english')\n",
        "train['text'] = train['text'].apply(lambda text: \" \".join([word for word in text.split() if word not in (stop)]))\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "train['text'] = train['text'].apply(lambda text: \" \".join([stemmer.stem(word) for word in text.split()]))\n",
        "\n",
        "# Lemmatizing\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "train['text'] = train['text'].apply(lambda text: \" \".join([lemmatizer.lemmatize(word) for word in text.split()]))\n"
      ],
      "metadata": {
        "id": "5tDnIMd-q89D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clean testing data**"
      ],
      "metadata": {
        "id": "-Npku_k2Tj5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text to lowercase\n",
        "test['text'] = test['text'].str.lower()\n",
        "\n",
        "# Expand contractions e.g \"gonna\" to \"going to\" and \"i've\" to \"i have\"\n",
        "test['text'].replace( {r\"`\": \"'\"}, inplace= True, regex = True)\n",
        "test['text'] = test['text'].apply(contractions.fix)\n",
        "\n",
        "# Remove @, Unicode characters, punctuation, emojis, URLs, retweets, words with digits, and 1 or 2 letter words\n",
        "test['text'].replace( {r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?|\\w*\\d\\w*|\\b\\w{1,2}\\b\": \" \"}, inplace= True, regex = True)\n",
        "\n",
        "# Remove extra whitespaces\n",
        "test['text'].replace( {r\" +\": \" \"}, inplace= True, regex = True)\n",
        "test['text'] = test['text'].str.strip()\n",
        "\n",
        "# Remove stopwords\n",
        "stop = stopwords.words('english')\n",
        "test['text'] = test['text'].apply(lambda text: \" \".join([word for word in text.split() if word not in (stop)]))\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "test['text'] = test['text'].apply(lambda text: \" \".join([stemmer.stem(word) for word in text.split()]))\n",
        "\n",
        "# Lemmatizing\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "test['text'] = test['text'].apply(lambda text: \" \".join([lemmatizer.lemmatize(word) for word in text.split()]))\n"
      ],
      "metadata": {
        "id": "dcgDVjv4TjSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train[\"text\"].head(10))\n",
        "print(train[\"text\"].tail(10))"
      ],
      "metadata": {
        "id": "tT-89PPBe703"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test['text'].head(20))\n",
        "print(test['text'].tail(20))"
      ],
      "metadata": {
        "id": "1C8JW4YmUcbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Feature Selection**"
      ],
      "metadata": {
        "id": "SRySCY_E8Kke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorization parameters\n",
        "# Range (inclusive) of n-gram sizes for tokenizing text.\n",
        "NGRAM_RANGE = (1, 2)  # Use 1-grams + 2-grams.\n",
        "\n",
        "# Limit on the number of features. We use the top 20K features.\n",
        "TOP_K = 20000\n",
        "\n",
        "# Whether text should be split into word or character n-grams.\n",
        "TOKEN_MODE = 'word' # Split text into word tokens.\n",
        "\n",
        "# Minimum document frequency below which a token will be discarded.\n",
        "MIN_DOCUMENT_FREQUENCY = 2\n",
        "\n",
        "def ngram_vectorize(train_texts, train_labels, test_texts):\n",
        "    \"\"\"Vectorizes texts as n-gram vectors.\n",
        "\n",
        "    1 text = 1 tf-idf vector the length of vocabulary of unigrams + bigrams.\n",
        "\n",
        "    # Arguments\n",
        "        train_texts: list, training text strings.\n",
        "        train_labels: np.ndarray, training labels.\n",
        "        test_texts: list, test text strings.\n",
        "\n",
        "    # Returns\n",
        "        train_vectors, test_vectors: vectorized training and test texts\n",
        "    \"\"\"\n",
        "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
        "    kwargs = {\n",
        "            'ngram_range': NGRAM_RANGE,\n",
        "            'analyzer': TOKEN_MODE,  \n",
        "            'min_df': MIN_DOCUMENT_FREQUENCY,\n",
        "            'sublinear_tf': 'True'\n",
        "    }\n",
        "    vectorizer = TfidfVectorizer(**kwargs)\n",
        "\n",
        "    # Learn vocabulary from training texts and vectorize training texts.\n",
        "    train_vectors = vectorizer.fit_transform(train_texts)\n",
        "\n",
        "    # Vectorize validation texts.\n",
        "    test_vectors = vectorizer.transform(test_texts)\n",
        "\n",
        "    # Select top 'k' of the vectorized features.\n",
        "    selector = SelectKBest(f_classif, k=min(TOP_K, train_vectors.shape[1]))\n",
        "    selector.fit(train_vectors, train_labels)\n",
        "    train_vectors = selector.transform(train_vectors).astype('float32').toarray()\n",
        "    test_vectors = selector.transform(test_vectors).astype('float32').toarray()\n",
        "    return train_vectors, test_vectors\n",
        "\n",
        "train_vectors, test_vectors = ngram_vectorize(train['text'], train['sentiment'], test['text'])\n",
        "\n",
        "\n",
        "# # Create feature vectors\n",
        "# vectorizer = TfidfVectorizer(min_df = 5,\n",
        "#                              max_df = 0.8,\n",
        "#                              sublinear_tf = True,\n",
        "#                              use_idf = True)\n",
        "# train_vectors = vectorizer.fit_transform(train['text'])\n",
        "# test_vectors = vectorizer.transform(test['text'])"
      ],
      "metadata": {
        "id": "I3KHg2k7kGyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Supervised modelling**"
      ],
      "metadata": {
        "id": "qSC6vhwKaAb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "supervised_models = [\n",
        "    LinearSVC(),\n",
        "    SVC(kernel='linear'),\n",
        "    XGBClassifier(objective='multi:softmax'),\n",
        "]\n",
        "\n",
        "# 5-fold Cross-validation\n",
        "k = 5\n",
        "cv_df = pd.DataFrame(index=range(k * len(supervised_models)))\n",
        "\n",
        "entries = []\n",
        "for model in supervised_models:\n",
        "  model_name = model.__class__.__name__\n",
        "  accuracies = cross_validate(model, train_vectors, train['sentiment'], scoring='accuracy', cv=k)\n",
        "  for fold_id, accuracy in enumerate(accuracies):\n",
        "    entries.append((model_name, fold_id, accuracy))\n",
        "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_id', 'accuracy'])"
      ],
      "metadata": {
        "id": "WKzE_70_F3dT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_accuracy = cv_df.groupby('model_name').accuracy.mean()\n",
        "std_accuracy = cv_df.groupby('model_name').accuracy.std()\n",
        "\n",
        "acc = pd.concat([mean_accuracy, std_accuracy, mean_f1, std_f1], axis= 1, ignore_index=True)\n",
        "acc.columns = ['Accuracy', ' Std dev']\n",
        "acc"
      ],
      "metadata": {
        "id": "TE3JzHbFR2lC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearSVC(loss='hinge', max_iter=1000)\n",
        "model.fit(train_vectors, train['sentiment'])\n",
        "prediction = model.predict(test_vectors)\n",
        "print(f\"Test set accuracy: {accuracy_score(test['sentiment'], prediction) * 100} %\\n\")"
      ],
      "metadata": {
        "id": "9iWtXl49SIad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report\n",
        "print('\\tClassification Metrics - LinearSVC\\n')\n",
        "print(classification_report(test['sentiment'], prediction, target_names= ['negative', 'neutral', 'positive']))"
      ],
      "metadata": {
        "id": "jfBEYCs06XWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = confusion_matrix(test['sentiment'], prediction)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=data, display_labels=model.classes_)\n",
        "disp.plot(cmap=\"Blues\")\n",
        "plt.ylabel('ACTUAL')\n",
        "plt.xlabel('\\nPREDICTED')\n",
        "plt.title(\"\\nCONFUSION MATRIX - LinearSVC\\n\");\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "35Inx_EO6nKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = XGBClassifier(objective='multi:softmax')\n",
        "model.fit(train_vectors, train['sentiment'])\n",
        "prediction = model.predict(test_vectors)\n",
        "print(f\"Test set accuracy: {accuracy_score(test['sentiment'], prediction) * 100} %\\n\")"
      ],
      "metadata": {
        "id": "mmrYfV82SwfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification report\n",
        "print('\\tCLASSIFICATIION METRICS - XGBClassifier\\n')\n",
        "print(classification_report(test['sentiment'], prediction, target_names= ['negative', 'neutral', 'positive']))"
      ],
      "metadata": {
        "id": "4J1rj4D6TAQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = confusion_matrix(test['sentiment'], prediction)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=data, display_labels=model.classes_)\n",
        "disp.plot(cmap=\"Blues\")\n",
        "plt.ylabel('ACTUAL')\n",
        "plt.xlabel('\\nPREDICTED')\n",
        "plt.title(\"\\nCONFUSION MATRIX - XGBClassifier\\n\");\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B-fd1cE6WRcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Unsupervised modelling**"
      ],
      "metadata": {
        "id": "fp5HLVdNboZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp_model(layers, units, op_units, op_activation, dropout_rate, input_shape, num_classes):\n",
        "    \"\"\"Creates an instance of a multi-layer perceptron model.\n",
        "\n",
        "    # Arguments\n",
        "        layers: int, number of `Dense` layers in the model.\n",
        "        units: int, output dimension of the layers.\n",
        "        dropout_rate: float, percentage of input to drop at Dropout layers.\n",
        "        input_shape: tuple, shape of input to the model.\n",
        "        num_classes: int, number of output classes.\n",
        "\n",
        "    # Returns\n",
        "        An MLP model instance.\n",
        "    \"\"\"\n",
        "\n",
        "    model = models.Sequential()\n",
        "    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n",
        "\n",
        "    for _ in range(layers-1):\n",
        "        model.add(Dense(units=units, activation='relu'))\n",
        "        model.add(Dropout(rate=dropout_rate))\n",
        "\n",
        "    model.add(Dense(units=op_units, activation=op_activation))\n",
        "    return model"
      ],
      "metadata": {
        "id": "lyYc_8N3bkb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train model**"
      ],
      "metadata": {
        "id": "0ujbGFwieJlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ngram_model(train_vectors, train_labels, test_vectors, test_labels,\n",
        "                      num_classes,\n",
        "                      learning_rate=1e-3,\n",
        "                      epochs=1000,\n",
        "                      batch_size=128,\n",
        "                      layers=2,\n",
        "                      units=64,\n",
        "                      dropout_rate=0.2):\n",
        "    \"\"\"Trains n-gram model on the given dataset.\n",
        "\n",
        "    # Arguments\n",
        "        data: tuples of training and test texts and labels.\n",
        "        learning_rate: float, learning rate for training model.\n",
        "        epochs: int, number of epochs.\n",
        "        batch_size: int, number of samples per batch.\n",
        "        layers: int, number of `Dense` layers in the model.\n",
        "        units: int, output dimension of Dense layers in the model.\n",
        "        dropout_rate: float: percentage of input to drop at Dropout layers.\n",
        "\n",
        "    # Raises\n",
        "        ValueError: If validation data has label values which were not seen\n",
        "            in the training data.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create model instance.\n",
        "    model = mlp_model(layers=layers, units=units, \n",
        "                      op_units=num_classes, op_activation = 'softmax',\n",
        "                      dropout_rate=dropout_rate,\n",
        "                      input_shape=train_vectors.shape[1:],\n",
        "                      num_classes=num_classes)\n",
        "\n",
        "    # Compile model with learning parameters.\n",
        "    loss = 'sparse_categorical_crossentropy' # for multiclass\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
        "\n",
        "    # Create callback for early stopping on validation loss. If the loss does\n",
        "    # not decrease in two consecutive tries, stop training.\n",
        "    callbacks = [EarlyStopping(monitor='val_loss', patience=2)]\n",
        "\n",
        "    # Train and test model.\n",
        "    history = model.fit(\n",
        "            train_vectors,\n",
        "            train_labels,\n",
        "            epochs=epochs,\n",
        "            callbacks=callbacks,\n",
        "            validation_data=(test_vectors, test_labels),\n",
        "            verbose=2,  # Logs once per epoch.\n",
        "            batch_size=batch_size)\n",
        "\n",
        "    # Print results.\n",
        "    history = history.history\n",
        "    print('Test accuracy: {acc}, loss: {loss}'.format(\n",
        "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
        "\n",
        "    # Save model.\n",
        "    model.save('Twitter_mlp_model.h5')\n",
        "    return history['val_acc'][-1], history['val_loss'][-1]"
      ],
      "metadata": {
        "id": "F5YNy00ieJCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = np.array(train['sentiment'])\n",
        "train_labels = []\n",
        "for i in range(len(labels)):\n",
        "    if labels[i] == 'neutral':\n",
        "        train_labels.append(0)\n",
        "    if labels[i] == 'negative':\n",
        "        train_labels.append(1)\n",
        "    if labels[i] == 'positive':\n",
        "        train_labels.append(2)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "labels = np.array(test['sentiment'])\n",
        "test_labels = []\n",
        "for i in range(len(labels)):\n",
        "    if labels[i] == 'neutral':\n",
        "        test_labels.append(0)\n",
        "    if labels[i] == 'negative':\n",
        "        test_labels.append(1)\n",
        "    if labels[i] == 'positive':\n",
        "        test_labels.append(2)\n",
        "test_labels = np.array(test_labels)"
      ],
      "metadata": {
        "id": "5i9e7-1Ys-7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_units=64\n",
        "\n",
        "train_ngram_model(train_vectors, train_labels, test_vectors, test_labels,\n",
        "                  num_classes=3,\n",
        "                  learning_rate=1e-3,\n",
        "                  epochs=1000,\n",
        "                  batch_size=128,\n",
        "                  layers=2,\n",
        "                  units=hidden_units,\n",
        "                  dropout_rate=0.2)"
      ],
      "metadata": {
        "id": "EjbkRki7hP07"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}